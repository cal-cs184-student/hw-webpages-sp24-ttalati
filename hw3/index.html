<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 184 Homework 3, 
Tanush Talati</title>
    <style>
	.left {
		text-align: left;
	}
        .caption {
            text-align: center;
            font-style: italic;
        }
        .im-container {
            display: flex; 
            justify-content: space-between; 
            margin-bottom: -10px;
            flex-wrap: wrap;
            text-align: center;
        }
        .label {
            text-align: center;
        }
        .image-container {
            width: 70%;
        }
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .image {
            width: 100%;
            margin-bottom: -10px;
        }
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            position: fixed;
            bottom: 0;
            width: 100%;
        }
        h1, h2, h3, h4, h5, h6 {
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <header>
        <h1>CS 184 Homework 3: Pathtracer</h1>
        <h2> 
Tanush Talati </h2>
    </header>
    
    <div class="container">
        <h2>Overview</h2>

	<p>
	This assignment required us to implement many of the core functionality of path-tracing based renderers. It was split into five subparts.
	</p>

	<p>
	In the first part ray-generation and simple intersection tests were implemented. Given that the rendering loop is trying to get an accurate picture of the light or the radiant intensity at each pixel, the idea is that we cast rays (the number depends on the sample size indicated) at each pixel unit-square and then use recursive ray-casting to ascertain the sum total of light being emitted from that point (the sum is of the direct and indirect light being reflected). Along the way to determine and book-keep which points our ray pass through, we need functionality to first generate rays from the camera space and second have intersection tests that can help determine if a ray passes through a certain polygon mesh component. 
	</p>

	<p>
	In the second part a Bounding Volume Hierarchy was implemented. This data structure helps immensely speed up ray-tracing times. Without a BVH once a ray is casted, all the polygons and primitives are looped through for an intersection test to determine which polygon the ray intersects and naturally which one is the first polygon in the scene. When scaling to many different polygons or high ray and sample counts, clearly this method is too slow; a faster intersection test is required. This is where the BVH shines. Essentially we create a tree-like structure where primitives have been partitioned into smaller bounding boxes. One can imagine that testing if the ray hits a larger bounding box, which represents multiple primitives can essentially allow us to test intersection on a range of primitives at once and recurse to the right range, eliminating extraneous tests. This is very akin to binary search on a range of values, essentially helping our intersection search times decrease from a linear order of magnitude into a logarithmic order, scaling with the number of primitives. 
	</p>

    <p>
	After much of the core machinery was built to allow us to implement ray-tracing, the next part actually focused on enabling rendering through light transport. Specifically, first BRDF functions were implemented for diffuse objects, which essentially indicates how the object reflects the light it receives outward, helping us know the radiance across each ray that is eventually traced. Next zero-bounce illumination and direct lighting estimates were implemented. Zero-bounce illumination was quite simple; we already knew where in the worldspace our intersection was and we just needed to query the BRDF function of that primitive. For direct-illumination, we needed to now generate a new ray from the intersection point out to the world. Usually to get an exact picture of the total light coming from another source to the intersection point, we would integrate over the entire hemisphere, but integration is clearly not feasible due to lack of closed-form solutions especially when we do multi-level ray-tracing, so instead Monte-Carlo sampling was used to estimate the value of the irradiance. Then after the irradiance was found, the BRDF function of the object helped actually return the amount of light transported in the direction of the incoming ray. At the end we made another realization. Specifically, instead of casting rays in all directions uniformly we could just cast a ray in a random direction toward a light source, this way noise of casting rays toward non-emitting sources could be reduced. Of course the pdf would be updated correspondingly to make all the sampling unbiased.
    </p>
    
    <p>
   After direct illumination was implemented, the next task was implementing global illumination. Essentially, in this task we generalize the 1-bounce nature in direct sampling to n-bounces, where n is an arbitrary user defined number. Clearly, this is a recursive process so the implementation was mainly setting up the recursive tools and correctly weighting the Monte-Carlo estimates to prevent bias. Essentially once you reach an intersection point you determine if you have any more bounces left, if you do you call the recursive function again by trying to see how much light is reflected into you from another light source. One observation in this method is that manually setting the number of bounces could itself lead to a biased rendering of the image. Taking that into account, the idea of russian roulette global illumination was implemented, where at each bounce there was a chance that no new rays would be cast. To implement this the existing code needed to be tweaked to stop early and we needed to remove a base case, which would be the natural stopping point in a deterministic recursive implementation.
    </p>

<p>

Finally, adaptive sampling was implemented. The idea behind adaptive sampling is that sometimes our estimates for the radiance in a particular direction at an intersection point converges quite quickly, hence we do not need to take too many samples to understand the light properties. In these cases we can save time, without the expense of compromising render quality by just moving on to the next pixel. So this was exactly what was implemented; the maximum number of samples that it could take was set to a very high value but once we determined that our interval of sampled light was within a 95% confidence interval we stopped sampling and assigned the pixel radiance to the mean of the sampled values. 
</p>
	    
        <section>
            <h3>Part 1: Ray Generation and Scene Intersection</h3>
            
            
            <p>The entire pipeline of ray generation is that we trace out rays from the camera’s perspective of the image that is being captured onto the world. The points at which we intersect in the world (with the first intersection on the ray being the only one that counts since the image will be 2 dimensional) will have some radiant energy. We want to capture the radiance along the camera ray that went to the scene to get a better picture of the lighting in the environment, allowing us to have a better rendering and rasterization of the scene. Now that we understand the motivation behind ray-tracing; it gives a more physically accurate model of lighting that can be implemented to render more realistic images, we need the functionality to actually generate rays from the camera space out to the world. To make calculations easier, the image of interest is transformed into camera space from normalized image space, from which we then cast out a ray to a particular pixel. This ray is then converted into world space to make the coordinates consistent with the rest of the calculations that occur later. The equations for the conversion from normalized image to camera were laid out in the spec.             
            </p>

		<p>One thing to note is that the boundary of a pixel is the range of coordinates from (x, y) to (x+1, y+1). This was the case in simple rasterization as well, so the idea is we want to cast multiple rays out uniformly in that region to get a better understanding of the radiance we want to rasterize our pixel with. These two things were implemented in the first two parts of the task.

        
            </p>

		<p>Next we actually dealt with implementing intersection tests with primitives. Particularly, the two primitives that we made intersection tests for were the sphere and triangle. The usage of these tests are wide: when we cast rays we want to understand the objects they intersect with in the world, which will allow us to then query for its light emittance and reflectance properties, depending on the ray that is being cast. These queries help us up the pipeline to actually determine the rasterized light intensity.           
            </p>

		<p>In implementing ray-triangle intersection, I utilized the Möller–Trumbore formula presented in lecture. Essentially this equation presents a closed form solution that given the ray (parametrized by the starting point and the vector direction it is growing in [note that all of this is now done in world space, which we convert into using the transformation matrices provided]) and the three vertices of the triangle, it return a matrix-vector relationship that returns a vector [t, beta, gamma], where beta and gamma are the barycentric coordinates of the intersection point of the ray on (right now) the plane that the three points lie on and t represents the value of t where the intersection lies in. As one can imagine, this equation will always give us a vector, but we must sanitize the vector to make sure that the intersection actually exists inside the triangle and inside the domain of ray values that we consider valid. To see if the intersection is inside the triangle, we simply use the property of barycentric coordinates, specifically if alpha (1 - beta - gamma), beta, and gamma are all between 0 and 1 and sum to 1 that means the intersection is inside the triangle. Secondly, if t is between the min_t and max_t parameters this is a valid intersection as well. We now also update max_t to be the t value of this intersection since we do not want objects behind the first object to count as intersection because in reality they are blocked (at least when the object is opaque). 
</p>

		<p>Ray-sphere intersection was structurally similar, but the intersection point formula was a bit different. Specifically we know that a sphere is defined by (p - c) - r^2 = 0. Where p are the set of points on the boundary of the sphere, c is the center, and r is the radius. To test where the ray intersections with the sphere we simply plug in p_r + d * t (the equation of our ray) into p and solve for t, the only constant remaining. As shown in the lecture slides, when expanding the equation once we have plugged in the equation for our ray, we get a quadratic equation, of which we need to find the roots. This can be then done using the quadratic formula, which utilizes the coefficients of the quadratic equation. At the end of this process, we have a (or multiple or none) value for t. We then again test the potential values, keeping only the minimum value for t that also satisfies the constraint of being within min_t and max_t. And like previously, we update max_t to be the new t value, if a satisfying one was found.
</p>

	    <div class='im-container'>
                <div>
                <div class="label"> CBspheres </div>
                <img class="image-container" src="CBspheres.png" alt="iter0">
                </div>
                <div>  
                <div class="label"> CBempty </div>    
                <img class="image-container" src="CBempty.png" alt="iter1">
                </div>
		<div>  
                <div class="label"> CBcoil </div>    
                <img class="image-container" src="blob.png" alt="iter2">
                </div>
            </div>

	<p class="caption">Showing some normal shading renders of scenes. This showcases a working intersection test and ray generation.
</p>
		
        </section>
        
        <section>
            <h3>Part 2: Bounding Volume Hierarchy </h3>
            <p> As noted in the overview, naively when testing whether or not a ray intersects a particular primitive, all the primitives are looped through and then we check for intersection on each one. This is linear in the order of the number of primitives, which could be pretty slow. This is where the BVH comes into play, a tree structure that helps cut the intersection test time to a logarithmic order, much more amenable to scaling. 
	    </p>
            <img class="image" src="teapot.png" alt="teapot">
 <p class="caption">The evaluated bezier surface which generates this teapot.
 </p>       
        </section>

        <section>
            <h3>Part 3: Area-Weighted Vertex Normals </h3>
<p> The main challenge with implementing the assignment of vertex normals to be the area-weighted normal vector of all the vertex’s neighboring faces was getting used to the half-edge data structure and coming up with a way to traverse and change pointers in a manner that would lead us to iterating over all of the neighboring faces of a particular vertex. The pattern of traversal that was deduced was start at the current half-edge of the instance vertex that is calling the function. Each half edge is associated with a face so we can query for that. If the face is a boundary face then it is a “virtual face” and we should not have that in our area calculations. If not, we need to update our running weighted sum. We describe the methodology to calculate the weights in a bit. After the weight calculation and sum update is done, pointers are changed in the following manner: we access the twin halfedge (so the halfedge that points into our vertex) and then access the next half edge of the twin in this way in our next iteration inside our loop logic, the face will be the neighboring face and our logic can continue. Due to the structure of the manifolds we are guaranteed to return back to our starting halfedge, which is our ending point of the loop. Once the loop ends, we simply return a normalized version of our running weighted normal vector sum, giving the weighted-normal of the target vertex.
</p>
		<p>Now we describe how to find the weight (which is the area of the triangular face). We know through vector algebra the norm of the cross product of two vectors gives the area of the parallelogram that is formed by those two vectors. Halving that value gives the area of the triangle specified by the two vectors (since two vectors and an angle can form a triangle [recall side angle side congruence]). To find these vectors we need 3 points, one being the vertex and the other 2 being the neighboring ones to form the vector-ray. These neighbors can be found by accessing the current half-edge’s next half-edge and querying for the vertex that it roots and also the current half-edge’s next-next vertex. With these 3 points, we just find the vector from vertex -> point 1 and vertex -> point 2, find the norm of the cross product and divide by 2 to get the area. Then we query for the actual norm of the face by simply using the member function of the Face class and multiply it by the calculated average and add it to our running sum. The final sum clearly is the weighted sum of all the neighboring face-norms, which can be used to find the final answer as indicated in the previous paragraph.
		</p>

	<div class='im-container'>
                <div>
                <div class="label"> Flat Shading </div>
                <img class="image-container" src="flatshading.png" alt="flatshading">
                </div>
                <div>  
                <div class="label"> Phong Shading </div>    
                <img class="image-container" src="phongshading.png" alt="phongshading">
                </div>
		<div>  
            </div>
	<p class="caption">Comparison of two different shading mechanisms used to compute shading on the teapot. Flat shading makes use of face normals. Phong Shading, which makes use of vertex normals, which we built the machinery to calculate in this part, results in a much more smooth surface visual.
</p>
            
        </section>

        <section>
            <h3>Part 4: Edge Flip</h3>

            <p>The spec provided some really helpful tips that made the implementation of the edge flip more painless. Specifically, stating that we should just reassign all of the pointers for all of structures involved (namely vertices, edges, half-edges, and faces) probably saved a lot of time compared to if we proceeded with the alternative option of trying to find our which structures had a change in pointers and only attempting to modify those. Similarly this advice motivated the idea of first assigning all the involved structures into a variable first and then mechanically reassigning the pointers and swapping things as deemed necessary.</p>

		<p>At a high level the edge flip is changing the vertices that connect a certain edge along the other two points out of the 4 points involved in the two triangles that share the edge. In terms of the actual mechanics of implementing the edge flip one important thing to notice is that no new structures are created to implement this functionality. Therefore, we just need to reassign existing ones. So once we listed out all the structures (vertices a, b, c, d; half-edges ac, cd, db, ba, cb, bc; edges ac, cd, db, ba; faces 1 and 2) on paper we drew out the post-flip state of our 2-triangle geometry and reassigned the appropriate changes. For instance the face of half-edge cd was changed to correspond to the face 1 instead of it being in face 2 as it was previously. Some other half edges also had changes to their next value, specifically the ones on the edge that was flipped. Because of these multiple changes, we just used the post-condition to completely reassign all the values of the pointers. Other structures that we had to modify were the face and vertex pointers. Here the modification was just to make sure that the half-edge they point to actually still corresponds to a half-edge that represents the face or vertex because those could be changed and we could have wrong pointers. The code provides a list of the exact changes but this was the gist of the main challenges of the problem. </p>

	 <div class='im-container'>
                <div>
                <div class="label"> Teapot Before Flips </div>
                <img class="image-container" src="before.png" alt="before">
                </div>
                <div>  
                <div class="label"> Teapot After Flipping One Diagonal Edge </div>    
                <img class="image-container" src="after1.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> Teapot After Flipping Multiple Vertical Edges </div>    
                <img class="image-container" src="after2.png" alt="after2">
                </div>
		<div> 
            </div>
	<p class="caption">Pictures showing the functional edge-flip mechanism.</p>

      <p class="left">Initially while implementing this functionality, I thought for some reason I could reasonably visualize all the pointer changes mentally and proceeded to write the code. Perhaps predictably one could imagine this did not bode well. Then—like I should have done earlier—I pulled out a pencil and paper, drew up the toy example and labeled the key structures both before and after the flip. This made life much easier as the work of the before and after stage was already on paper and I just had to carefully replicate it in code now. Attached for no real purpose is the pen-paper masterpiece.</p>

 <img class="image" src="edgeflipdebug.jpeg" alt="edgeflipdebug">
 <p class="caption">Although the pictures are not too detailed, having a reference to look to helps speed up programming as well as reduces the possibility of errors from my empirical evidence. 
 </p>     

        </section>

        <section>
             <h3>Part 5: Edge Split</h3>
            <p> In a nutshell, in the edge split operation, we attach a new vertex at the midpoint of the edge we are trying to split and then create an edge in the same direction as if we were trying to flip the selected edge. Now the original two triangles have been partitioned into four separate triangles. This means there are 2 new faces (since we have two new triangles), one new vertex (the midpoint), three new edges (across the midpoint), and also six new half-edges. Other than that the rest of the challenge is just reassigning pointers in the old structures to either the new structures that are created through the split or to a new location as a result of the split. Therefore, the edge split follows much of the same basic steps as the flip except we are also adding new structures. Specifically, to tackle the split, like with the flip we first initialized variables for all the structures involved  (vertices a, b, c, d; half-edges ac, cd, db, ba, cb, bc; edges ac, cd, db, ba; faces 1 and 2). Then initialize the new structures that will be introduced post flip (vertex m; faces 3, 4; 6 new half-edges, 3 new edges). Once all of these structures were initialized, the task becomes the same as in the flip-case: correctly populate the field variables of the structures. The most complicated assignments are for the half-edges, but working through a drawn diagram can allow us to systematically label the next half-edge, face, vertex, and twin pointers. Then we simply need to make sure that each of the face, vertex, and edge structures have a correct pointer to the right half-edge. Candidly, there is no real smart algorithm, these pointer adjustments just have to be carefully assigned by just assigning as the drawing of the post-split state indicates. 
</p>
<p>
One thing that helped make this process much smoother was actually giving nice names to the structures instead of the rather unreadable and uninterpretable names we gave them when doing the flip operation implementation. Specifically I labeled the vertices a, b, c, d and edges similarly instead of just using numbers. This way I had a much easier time translating the drawn image state into the actual code that implements that logic. 
</p>
            <div class='im-container'>
                <div>
                <div class="label">Teapot Before Splits </div>
                <img class="image-container" src="tp.png" alt="tp">
                </div>
                <div>  
                <div class="label"> Teapot After One Split </div>    
                <img class="image-container" src="tpflip.png" alt="tpflip">
                </div>
                <div>
                <div class="label"> Teapot After Many Splits </div>
                <img class="image-container" src="tpflips.png" alt="tpflips">
                </div>
            </div>
            <p class="caption">Showing the edge-flip functionality. Note how the last image has multiple splits within one triangle as well.
</p>
           <div class='im-container'>
                <div>
                <div class="label">Teapot Before changes </div>
                <img class="image-container" src="first.png" alt="first">
                </div>
                <div>  
                <div class="label"> Teapot After Applying Initial Splits </div>    
                <img class="image-container" src="second.png" alt="second">
                </div>
                <div>
                <div class="label"> Teapot After Applying Flips to Splits </div>
                <img class="image-container" src="third.png" alt="third">
                </div>
		<div>
                <div class="label"> Teapot After Applying Splits to Flips of Splits </div>
                <img class="image-container" src="last.png" alt="last">
                </div>
            </div>
            <p class="caption">This sequence of photos shows the use of a  functional half-edge split with a functional half-edge flip to create complex transformations.
</p>

		<p>Initially while trying to implement half-edge splitting I had quite unreadable variable names, which did not help in making my job of deciding where pointers lie easier. After trying to get this version working, I decided to give the variables more descriptive names (corresponding with the actual labels I had on my paper diagram). This made it much faster and easier to implement the splitting function. 
</p>
  <img class="image" src="edgesplitdebug.jpeg" alt="edgesplitdebug">
		<p class="caption">Note how again the drawing is not too involved but goes a long way in making programming easier.
</p>
            <h4 class="left">Extra Credit: Boundary Edge Split</h4>
<p class="left"> I implemented the edge-split functionality on boundary edges. Below are the photos of it working</p>
	<div class='im-container'>
                <div>
                <div class="label"> Beetle Car Before </div>
                <img class="image-container" src="carbefore.png" alt="before">
                </div>
                <div>  
                <div class="label"> Applying Boundary Edge Splits with Other Edge Splits </div>    
                <img class="image-container" src="carafter.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> Applying Multiple Boundary Edge Splits </div>    
                <img class="image-container" src="multiple.png" alt="after2">
                </div>
		<div> 
            </div>
	<p class="caption">Pictures showing the functional boundary edge-flip mechanism.</p>
	<p class="left">The implementation was not too bad as I had a lot of pointer manipulation practice from the regular edge-split and edge-flip implementation. The subtly to note is that only one new triangle inside is created and there is one new half-edge that exists on the boundary. After noting those, the steps that proceed are the same as just carefully reassigning pointers in any other case.</p>
        </section>

        <section>
             <h3>Part 6: Loop Subdivision for Mesh Upsampling</h3>

        <p>
	Basically loop subdivision is a method to upsample our given mesh and as mentioned in the overview, relies heavily on our edge-split and flip functionality we worked on. The mechanics of the algorithm to implement mesh upsampling were pretty finely detailed in the spec so it was just a matter of correctly translating those steps into code. Specifically the complexity of trying to upsample lies in two factors: introducing new structures as well as changing the positions of our old structures. However, some of the properties of the new structures depend on the positions and properties of the old ones, which made it important to be careful in the order that changes were made. Specifically, each of the structures had a field that stored the old and new positions as well as a boolean flag which labeled whether or not the structure itself was new or not. Then we computed the new positions of the old vertex (storing into the old vertices’ field) as well as the position of new vertices (which would go inside the edge that would be split to achieve that new vertex). Both the position of the new position of the old vertex and the position of the new vertex were calculated according to the loop division formula that was given. While the subdivision and the edge split and flip of the triangle were described earlier it is easier to calculate out all the positions and then deal with the geometric flipping and splitting that would place the right structures at the right location.	
	</p>

		<p>Next we would split every edge and once we split the edge, utilizing the edge split subroutine. While doing this we propagate over the position of the new vertex from the value that was stored inside the edge structure previously. One thing that we had to be careful about was not splitting newly created edges, but that could be determined easily (if either of the vertices that make up the edge were newly created then the edge is new and we ignore it). 
		</p>

	<p>
	After edge splitting was done, we then flipped edges that connected one new vertex with another old one (also done through using a simple xor on the vertex-is-new flags). And once we determined that an edge needed to be flipped we simply called the flipEdge function we created earlier. Another subtlety here is we had to make sure to only flip new edges since old edges should not be flipped no matter the state of the vertex, which could also be determined by a simple conditional statement on the flags.	
	</p>

	<p>
	Finally we have the mesh ready and just need to make the actual position field for all the vertices the one that is stored in their corresponding newPosition field, completing our upsampling. 	
	</p>

	<p>
	When looking at the behavior of sharp corners and edges under loop subdivision, it is evident these sharp parts of the geometry have a much harsher/larger change at each level of upsample than non sharp corners and edges. Specifically compared to the original geometry, the upsampled version pulls in these corners and edges much more in an effort to make the overall geometry more rounded. Since sharp corners and edges see a much larger inward pull effect than non sharp ones, we disproportionately change the geometry or shrink the sharper areas compared to the non-sharper ones, leading to asymmetry. This also makes sense as in sharper corners, the locations of the other vertices in comparison to ours are much more different, so when we apply the weighted averaging step, we are going to see a large change in the new location of our vertex in the upsampled version, whereas in non-sharp corners and edges our change is not going to be as drastic. In either case our figure does get smaller but that change slows down at larger iterations of upsampling. One reason why we have this disproportionate figure is because the vertices are unevenly distributed. If we were to introduce symmetry on the locations and spacing of the vertex in our figure through splits, we can make our upsampling effect be the same throughout the geometry of our figure, curtailing the asymmetry issue. I show this in the example with fixing the cube.
	</p>
		
<div class='im-container'>
                <div>
                <div class="label"> Level 0 Original Cube </div>
                <img class="image-container" src="lvl0.png" alt="lvl0">
                </div>
                <div>  
                <div class="label"> Level 1 Upscaling </div>    
                <img class="image-container" src="lvl1.png" alt="lzpl">
                </div>
                <div>
                <div class="label"> Level 2 Upscaling </div>
                <img class="image-container" src="lvl2.png" alt="lnpn">
                </div>
                 <div>
                 <div class="label"> Level 3 Upscaling </div>
                <img class="image-container" src="lvl3.png" alt="lnpl">
                </div>
		<div>
                 <div class="label"> Level 4 Upscaling </div>
                <img class="image-container" src="lvl4.png" alt="lnpl">
                </div>
        </div>
        <p class="caption">Comparing the various levels of upscaling it is clear that sharp corners have a much larger inward movement effect compared to non sharp ones when we upscale. Due to this disproportionate movement, we see the eventual asymmetry. 
</p>

		<p>One thing I did to make the cube divide more symmetrically is make each of the faces the sample geometrically by splitting an edge on every face. This way the sharpness of the corners decreases as neighboring vertices are closer to each other and two all vertices will have similar neighbor topologies. One can imagine that increasing the number of triangles that make up our cube symmetrically will do an even better job at maintaining symmetry through various upsampling levels.
</p>
		<div class='im-container'>
                <div>
                <div class="label"> Level 0 Modified Cube </div>
                <img class="image-container" src="c0.png" alt="lvl0">
                </div>
                <div>  
                <div class="label"> Level 1 Upscaling </div>    
                <img class="image-container" src="c1.png" alt="lzpl">
                </div>
                <div>
                <div class="label"> Level 2 Upscaling </div>
                <img class="image-container" src="c2.png" alt="lnpn">
                </div>
                 <div>
                 <div class="label"> Level 3 Upscaling </div>
                <img class="image-container" src="c3.png" alt="lnpl">
                </div>
		<div>
                 <div class="label"> Level 4 Upscaling </div>
                <img class="image-container" src="c4.png" alt="lnpl">
                </div>
        </div>
		 <p class="caption">Notice how my changes helped improve upscale symmetry.
</p>
		<h4 class="left">Extra Credit: Support meshes with boundary</h4>
		<p class="left">In implementing supporting upscaling on the boundary at the boundary, I first looked up the rules for assigning new vertex (on the edge structure) and old vertex positions (on the vertex structure itself). This was found in the lecture slides on the lecture for Mesh Processing. Specifically the new position for old vertices corresponds to ⅛ * (sum of adjacent boundary vertex on boundary edges) + ¾ * (current vertex position). Similarly the update rule for a new vertex position is quite simple ½ * (sum of vertex positions that make up the edge). Once I knew these rules, much of the skeleton to add them in our upscale method already existed. Specifically I only had to add conditionals in the existing for loops that calculated new positions for old vertices and also the new positions on edges so they can be set on new vertices. The conditional corresponds to whether or not the edge or vertex I am dealing with is a boundary edge. If it was then we apply our new special logic. The application of the logic was quite straightforward as we just did simple pointer traversal and summing like previously implemented. With these changes, support for boundary upscaling was implemented. 
</p>
			<div class='im-container'>
                <div>
                <div class="label"> Level 0 Beetle Car </div>
                <img class="image-container" src="normalbeetle.png" alt="lvl0">
                </div>
                <div>  
                <div class="label"> Level 1 Upscaling on Boundary </div>    
                <img class="image-container" src="lvl1boundary.png" alt="boundary">
                </div>
                <div>
                <div class="label"> Level 2 Upscaling on Boundary </div>
                <img class="image-container" src="lvl2boundary.png" alt="boundary2">
                </div>		
        </section>
			<p class="caption">Notice how boundary meshes are also being upscaled.
</p>

<!-- 
			<table>
                <thead>
                    <tr>
                        <th>Column 1</th>
                        <th>Column 2</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Data 1</td>
                        <td>Data 2</td>
                    </tr>
                    <tr>
                        <td>Data 3</td>
                        <td>Data 4</td>
                    </tr>
                </tbody>
            </table> -->
        
        <!-- Add more sections as needed -->
        
    </div>
</body>
</html>
