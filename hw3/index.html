<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 184 Homework 3, 
Tanush Talati</title>
    <style>
	.left {
		text-align: left;
	}
        .caption {
            text-align: center;
            font-style: italic;
        }
        .im-container {
            display: flex; 
            justify-content: space-between; 
            margin-bottom: -10px;
            flex-wrap: wrap;
            text-align: center;
        }
        .label {
            text-align: center;
        }
        .image-container {
            width: 70%;
        }
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .image {
            width: 100%;
            margin-bottom: -10px;
        }
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            position: fixed;
            bottom: 0;
            width: 100%;
        }
        h1, h2, h3, h4, h5, h6 {
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <header>
        <h1>CS 184 Homework 3: Pathtracer</h1>
        <h2> 
Tanush Talati </h2>
    </header>
    
    <div class="container">
        <h2>Overview</h2>

	<p>
	This assignment required us to implement many of the core functionality of path-tracing based renderers. It was split into five subparts.
	</p>

	<p>
	In the first part ray-generation and simple intersection tests were implemented. Given that the rendering loop is trying to get an accurate picture of the light or the radiant intensity at each pixel, the idea is that we cast rays (the number depends on the sample size indicated) at each pixel unit-square and then use recursive ray-casting to ascertain the sum total of light being emitted from that point (the sum is of the direct and indirect light being reflected). Along the way to determine and book-keep which points our ray pass through, we need functionality to first generate rays from the camera space and second have intersection tests that can help determine if a ray passes through a certain polygon mesh component. 
	</p>

	<p>
	In the second part a Bounding Volume Hierarchy was implemented. This data structure helps immensely speed up ray-tracing times. Without a BVH once a ray is casted, all the polygons and primitives are looped through for an intersection test to determine which polygon the ray intersects and naturally which one is the first polygon in the scene. When scaling to many different polygons or high ray and sample counts, clearly this method is too slow; a faster intersection test is required. This is where the BVH shines. Essentially we create a tree-like structure where primitives have been partitioned into smaller bounding boxes. One can imagine that testing if the ray hits a larger bounding box, which represents multiple primitives can essentially allow us to test intersection on a range of primitives at once and recurse to the right range, eliminating extraneous tests. This is very akin to binary search on a range of values, essentially helping our intersection search times decrease from a linear order of magnitude into a logarithmic order, scaling with the number of primitives. 
	</p>

    <p>
	After much of the core machinery was built to allow us to implement ray-tracing, the next part actually focused on enabling rendering through light transport. Specifically, first BRDF functions were implemented for diffuse objects, which essentially indicates how the object reflects the light it receives outward, helping us know the radiance across each ray that is eventually traced. Next zero-bounce illumination and direct lighting estimates were implemented. Zero-bounce illumination was quite simple; we already knew where in the worldspace our intersection was and we just needed to query the BRDF function of that primitive. For direct-illumination, we needed to now generate a new ray from the intersection point out to the world. Usually to get an exact picture of the total light coming from another source to the intersection point, we would integrate over the entire hemisphere, but integration is clearly not feasible due to lack of closed-form solutions especially when we do multi-level ray-tracing, so instead Monte-Carlo sampling was used to estimate the value of the irradiance. Then after the irradiance was found, the BRDF function of the object helped actually return the amount of light transported in the direction of the incoming ray. At the end we made another realization. Specifically, instead of casting rays in all directions uniformly we could just cast a ray in a random direction toward a light source, this way noise of casting rays toward non-emitting sources could be reduced. Of course the pdf would be updated correspondingly to make all the sampling unbiased.
    </p>
    
    <p>
   After direct illumination was implemented, the next task was implementing global illumination. Essentially, in this task we generalize the 1-bounce nature in direct sampling to n-bounces, where n is an arbitrary user defined number. Clearly, this is a recursive process so the implementation was mainly setting up the recursive tools and correctly weighting the Monte-Carlo estimates to prevent bias. Essentially once you reach an intersection point you determine if you have any more bounces left, if you do you call the recursive function again by trying to see how much light is reflected into you from another light source. One observation in this method is that manually setting the number of bounces could itself lead to a biased rendering of the image. Taking that into account, the idea of russian roulette global illumination was implemented, where at each bounce there was a chance that no new rays would be cast. To implement this the existing code needed to be tweaked to stop early and we needed to remove a base case, which would be the natural stopping point in a deterministic recursive implementation.
    </p>

<p>

Finally, adaptive sampling was implemented. The idea behind adaptive sampling is that sometimes our estimates for the radiance in a particular direction at an intersection point converges quite quickly, hence we do not need to take too many samples to understand the light properties. In these cases we can save time, without the expense of compromising render quality by just moving on to the next pixel. So this was exactly what was implemented; the maximum number of samples that it could take was set to a very high value but once we determined that our interval of sampled light was within a 95% confidence interval we stopped sampling and assigned the pixel radiance to the mean of the sampled values. 
</p>
	    
        <section>
            <h3>Part 1: Ray Generation and Scene Intersection</h3>
            
            
            <p>The entire pipeline of ray generation is that we trace out rays from the camera’s perspective of the image that is being captured onto the world. The points at which we intersect in the world (with the first intersection on the ray being the only one that counts since the image will be 2 dimensional) will have some radiant energy. We want to capture the radiance along the camera ray that went to the scene to get a better picture of the lighting in the environment, allowing us to have a better rendering and rasterization of the scene. Now that we understand the motivation behind ray-tracing; it gives a more physically accurate model of lighting that can be implemented to render more realistic images, we need the functionality to actually generate rays from the camera space out to the world. To make calculations easier, the image of interest is transformed into camera space from normalized image space, from which we then cast out a ray to a particular pixel. This ray is then converted into world space to make the coordinates consistent with the rest of the calculations that occur later. The equations for the conversion from normalized image to camera were laid out in the spec.             
            </p>

		<p>One thing to note is that the boundary of a pixel is the range of coordinates from (x, y) to (x+1, y+1). This was the case in simple rasterization as well, so the idea is we want to cast multiple rays out uniformly in that region to get a better understanding of the radiance we want to rasterize our pixel with. These two things were implemented in the first two parts of the task.

        
            </p>

		<p>Next we actually dealt with implementing intersection tests with primitives. Particularly, the two primitives that we made intersection tests for were the sphere and triangle. The usage of these tests are wide: when we cast rays we want to understand the objects they intersect with in the world, which will allow us to then query for its light emittance and reflectance properties, depending on the ray that is being cast. These queries help us up the pipeline to actually determine the rasterized light intensity.           
            </p>

		<p>In implementing ray-triangle intersection, I utilized the Möller–Trumbore formula presented in lecture. Essentially this equation presents a closed form solution that given the ray (parametrized by the starting point and the vector direction it is growing in [note that all of this is now done in world space, which we convert into using the transformation matrices provided]) and the three vertices of the triangle, it return a matrix-vector relationship that returns a vector [t, beta, gamma], where beta and gamma are the barycentric coordinates of the intersection point of the ray on (right now) the plane that the three points lie on and t represents the value of t where the intersection lies in. As one can imagine, this equation will always give us a vector, but we must sanitize the vector to make sure that the intersection actually exists inside the triangle and inside the domain of ray values that we consider valid. To see if the intersection is inside the triangle, we simply use the property of barycentric coordinates, specifically if alpha (1 - beta - gamma), beta, and gamma are all between 0 and 1 and sum to 1 that means the intersection is inside the triangle. Secondly, if t is between the min_t and max_t parameters this is a valid intersection as well. We now also update max_t to be the t value of this intersection since we do not want objects behind the first object to count as intersection because in reality they are blocked (at least when the object is opaque). 
</p>

		<p>Ray-sphere intersection was structurally similar, but the intersection point formula was a bit different. Specifically we know that a sphere is defined by (p - c) - r^2 = 0. Where p are the set of points on the boundary of the sphere, c is the center, and r is the radius. To test where the ray intersections with the sphere we simply plug in p_r + d * t (the equation of our ray) into p and solve for t, the only constant remaining. As shown in the lecture slides, when expanding the equation once we have plugged in the equation for our ray, we get a quadratic equation, of which we need to find the roots. This can be then done using the quadratic formula, which utilizes the coefficients of the quadratic equation. At the end of this process, we have a (or multiple or none) value for t. We then again test the potential values, keeping only the minimum value for t that also satisfies the constraint of being within min_t and max_t. And like previously, we update max_t to be the new t value, if a satisfying one was found.
</p>

	    <div class='im-container'>
                <div>
                <div class="label"> CBspheres </div>
                <img class="image-container" src="CBspheres.png" alt="iter0">
                </div>
                <div>  
                <div class="label"> CBempty </div>    
                <img class="image-container" src="CBempty.png" alt="iter1">
                </div>
		<div>  
                <div class="label"> CBcoil </div>    
                <img class="image-container" src="blob.png" alt="iter2">
                </div>
            </div>

	<p class="caption">Showing some normal shading renders of scenes. This showcases a working intersection test and ray generation.
</p>
		
        </section>
        
        <section>
            <h3>Part 2: Bounding Volume Hierarchy </h3>
            <p> As noted in the overview, naively when testing whether or not a ray intersects a particular primitive, all the primitives are looped through and then we check for intersection on each one. This is linear in the order of the number of primitives, which could be pretty slow. This is where the BVH comes into play, a tree structure that helps cut the intersection test time to a logarithmic order, much more amenable to scaling. 
	    </p>

		<p>Constructing the BVH followed this process: namely the actual iterators for the primitives only exist at the leaf nodes. A node of the tree would become a leaf if the number of ongoing primitives were less than or equal to the maximum leaf size. This would basically function as our stopping point for recursion (or iteration as implemented in the extra credit) for creating new nodes in our BVH. The bulk of the logic is as follows for deciding the set of primitives that would belong to the right and left BVH nodes respectively. Basically given the root BVH node with a start and end iterator representing all the primitives that its bounding box covers, our job is to split it into two separate bounding boxes of primitives. Note that unlike KD-trees the bounding boxes are not disjoint but rather the primitives that are split are disjoint in a BVH. The splitting is determined by a heuristic. Initially just to check the functionality of the BVH I just split on the mean centroid of the x-axis but later I implemented a better heuristic. The heuristic that was used is as follows: we check all the three axis (x, y, z) and our candidate split point for each of the axis is the mean centroid location (so the mean of all the centroid locations for all the primitives along the axis of interest). On each of these candidates, I would check the surface area heuristic score. As introduced in lecture, essentially the score tries to minimize the chance of needless intersection tests. A heuristic for that determination is simply calculating the surface area of the bounding boxes of each split and multiplying it by the number of primitives in the split. The idea being that the number of intersection tests is proportional to the surface area of our split so minimizing it will reduce the intersections in the long run since the surface area encodes the probability of a ray hitting a bounding box node. Out of all the three axis, I kept the best (lowest) scoring split.  Once the split was determined, the goal was to actually add new child nodes and connect pointers to the parent. The parent node is confirmed not to be a leaf so it can now hold the pointers to the two new child pointers we create. Now the child nodes need an updated view of the iterators. These iterators were determined by editing the pointers that our iterators pointed to using a for loop that went through the new split primitives in order. This took a lot of time since initially just had one stack and set the iterators in the nodes but because the variables were declared as constants it would not allow me. I had to then use a 3-stack method (for the extra credit). Now that we have updated pointers and field variables, we can rely on the recursive base case to stop further iterating, once we have few primitives or no more actual splits (this was determined by checking if the array size of either the left or right split was zero, implying that we did not actually split [not possible with the heuristics we used]).	
		</p>

		<p>
		Once the BVH construction is complete, whenever we have an intersection task, the data structure is used as a way to get fast, logarithmic search. Namely two methods intersect and has_intersect were constructed. has_intersect just concerns with if there is an intersection not where it is so one can imagine the logic is the same as intersect except we can return early since we are not worried about finding the first intersection, rather just knowing if there is an intersection or not. Therefore, I will talk about the intersect method. Here we first check if the ray even intersects the root node, if not we know it cannot intersect any of the children either so it allows us to not do any redundant intersection tests. If we do have a bounded box intersection, we simply check if the node is a child, because if it is then we check the intersections on the primitives, which was implemented in the previous part and we will get the first primitive that intersected. If not we run this recursive intersection method on the two children, and the recursive step will guarantee us knowing whether or not an intersection has occurred and if it has the intersection structure will have the right populated fields. 	
		</p>

		<p>Here are large dae files that would take a painfully long time to render with no acceleration. With acceleration all of them took easily less than .1 seconds, showcasing the massive speedup.
		</p>
       
		<div class='im-container'>
                <div>
                <div class="label"> Dragon </div>
                <img class="image-container" src="dragon.png" alt="iter0">
                </div>
                <div>  
                <div class="label"> Max Planck </div>    
                <img class="image-container" src="max.png" alt="iter1">
                </div>
		<div>  
                <div class="label"> Lucy </div>    
                <img class="image-container" src="lucy.png" alt="iter2">
                </div>
		<div>
		<div class="label"> Wall-E </div>    
                <img class="image-container" src="empty.png" alt="iter2">
                </div>
            </div> 

	<p class="caption">All of these files are quite large so trying to render them without a BVH would take a lot longer.
</p>

	<p class="left">
		In this section I give some of the runtimes of attempting to render without BVH and with BVH, showcasing the large speedup the data structure brings. 
	</p>

	<table>
                <thead>
                    <tr>
                        <th>File Name</th>
                        <th>Non-BVH (seconds)</th>
			<th>BVH (seconds)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>cow.dae</td>
                        <td>3.223</td>
			<td>0.053</td>
                    </tr>
                    <tr>
                        <td>maxplanck.dae</td>
                        <td>36.4196</td>
			<td>0.071</td>
                    </tr>
		  <tr>
                        <td>dragon.dae</td>
                        <td>87.016</td>
			<td>0.088</td>
                    </tr>
                </tbody>
            </table>
<p>It is pretty clear that having the BVH structure makes a large difference. The magnitude is 100x better even for smaller renders and obviously the speedup differential will continue to increase as the renders get larger and larger as the underlying difference between log x vs x in growth will only continue to increase. This large gain in speedup is massive because it allows us to do more intensive and more intersection tests in general and is one reason that the render times for having multiple samples and also multiple light samples for each pixel sample and then also adding the fact that we will have more bounces instead of just direct lighting is even possible, otherwise if our intersection tests were too slow, at each step we are adding this multiplicative factor of iterations and all of them depend on a fast intersection resolution protocol.</p>
	
        </section>

        <section>
            <h3>Part 3: Direct Illumination</h3>
<p> In this part direct illumination was implemented. Specifically, both one-bounce lighting and zero-bounce lighting tracing was implemented. Lastly, two types of direct illumination were implemented and compared: uniform hemisphere sampling and light importance sampling.</p>
		<p>Before implementing any sort reflectance measure, first we need to define the BSDF of our primitives. In this assignment all of the surfaces were taken as diffuse and hence we needed to implement the BSDF function that would reflect the irradiance at a point equally in all directions across the hemisphere. This is important in knowing how much light travels toward the camera or later toward a bounce in the direction that is being sampled. Once the BSDF function was implemented the first thing done was to implement the zero-bounce illumination. This means that only emissive sources would render. In the case of the assignment, the light source at the top of the Cornell box. This was pretty easy to implement as when the zero_bounce function is called, we simply return the object’s emission, which is a field variable. Upstream, that will just get set to the pixel’s radiance (or be a factor in it when we do multiple bounces).
		</p>
		<p>Once the bare was finished, the task was to actually implement direct lighting. In this case we make use of the function: one_bounce_radiance. Basically this returns the amount of light reflected on the particular intersection point as a result of the direct illumination of some light source. The implementation of the function was such, we repeat the same process for the user-specified number of samples per light area. And just to be consistent with the other method of direct light sampling, we multiply the number of samples by the number of light sources so in both cases we sample equally. But back to talking about uniform hemisphere sampling, inside the loop we already know the intersection point so our goal is to take the number of sample light measurements from that point by casting that many rays from that point onto a hemisphere. In this first method, the casted ray is uniformly sampled. So in summary we have num_samples * light_sources samples that we take of a casted ray from the intersection point. Now we measure where that casted ray intersects out in the world. This is a bit tricky, since first the casted ray that we sampled was in object space but intersections are in world space, so we convert the ray into world space and then use our intersection machinery we developed to see if it intersects an object in our world. Another thing we want to be careful about is to set the minimum value for the intersection to be a bit greater than 0 because we do not want the intersection with the point we are casting the ray from to count. Once we have received our t value for the intersection (and a boolean indicating whether or not intersection occurred), if the intersection in the world occurred and because we know where, through the mechanics of the light transport we know that the amount of light emitted by the intersecting object will be the amount transferred into our object. Then using the BSDF function we can determine finally how much of that light is reflected onto the camera. The idea of sampling is a Monte-Carlo approximation of the integral of all the light in the whole world, and because we are taking samples to approximate that average integral value, we must make sure to normalize by the probability of casting that sample. In this case it was uniform on the hemisphere hence 1/2*pi. The sum that is getting accumulated is the total light transferred to the camera, which is just the product of the bsdf function * light transferred from the ray cast from intersection point to intersection point * cos (angle between casted ray and surface normal). The cosine is there because that gives the scalar light intensity quantity into the surface. At the end we divide the accumulating sum (our estimate of the integral) by the number of samples. Also if there was no intersection from the casted ray notice how the accumulating sum just gets added with 0 which is what the appropriate behavior should be.
		</p>
		<p>The previous section covered uniform hemisphere sampling, where once again we basically casted rays from the point of intersection uniformly in a hemisphere. But as I show in the pictures, this leads to a lot of noise as many directions just do not end up going to a light source, resulting in point darkness throughout the photos. Basically the idea behind light sampling is that we first iterate through all the light sources and for each light source we take num_samples ray samples uniformly in the region of the light source. This way we are guaranteed to be casting a ray toward a light source (this does not guarantee a hit of the light source because as we will see the light source could be blocked). Another tricky part is that some light sources are point light sources, that means only one direction exists. This is okay we can simply remove redundant computation by taking our sampled light and multiplying it by num_samples. The rest of the machinery should be similar. Now I will go through some implementation details that we need to be careful about when implementing this feature. When we iterate though each of the light sources first see if it is a point light source or not (if it is the former we can save computation as discussed above). Now that we have a sampled direction, we do the same as in the previous part of converting the direction to a world-space ray. We already know that this ray will hit the light eventually because we sampled in that way, but it may potentially hit another object before, in that case the light source is blocked and we should have a shadow and not add the effect of the light onto the light at our point. To make sure of this we set the bounds of the ray to be minimum a little greater than 0 (same reason as before to make sure we do not intersect the point we are casting the ray from) and the max bounds is a little less than the distance to the ray. Now if there is an intersection it means we DO NOT add the light sampled into our sum since that means we hit something in between. Otherwise just as before we update our Monte-Carlo estimate and make sure to remove the bias as well. At the end similar to the previous case we divide the sum by the total number of samples.
		</p>
		<p>Now I present some renderings using uniform sampling and direct-lighting sampling.
		</p>

	<div class='im-container'>
                <div>
                <div class="label"> Uniformed Sampled Bunny </div>
                <img class="image-container" src="unif_bunny.png" alt="iter0">
                </div>
                <div>  
                <div class="label"> Uniformed Sampled Spheres </div>    
                <img class="image-container" src="max.png" alt="iter1">
                </div>
		<div>  
                <div class="label"> Direct-Light Sampled Bunny </div>    
                <img class="image-container" src="bunny_l.png" alt="iter2">
                </div>
		<div>
		<div class="label"> Direct-Light Sampled Spheres </div>    
                <img class="image-container" src="spheres_L.png" alt="iter2">
                </div>
		<div>
		<div class="label"> Direct-Light Sampled Dragon </div>    
                <img class="image-container" src="dragon_L.png" alt="iter2">
                </div>
            </div> 
	<p class="caption">Here are some comparisons of direct-light sampling and uniform sampling. Clearly we see a lot more noise when we do uniform sampling.
</p>
		<p class="left">In comparing the uniform hemisphere sampling and the lighting sampling it is pretty clear that when all parameters are equal (sample numbers and light samples) lighting sampling has a much more clear rendering of the image. Even for pretty high sample values in both pixels and lights, uniform sampling produces quite noisy images, because we are sampling the whole hemisphere from a surface point, which naturally means a lot of non-collisions with light. If we just directly sample toward light sources and then adjust the pdf accordingly we are inherently reducing our sampling noise without compromising the bias of our sample. And we can see the results are much more clear, with very little noise and sharper images since we are directly sampling at a light source. </p>

		<p class="left"> Now I present some images of the effect of the number of light samples on rendering quality using direct light sampling. We use 1 sample per pixel while increasing the number of light samples. </p>
	
	<div class='im-container'>
                <div>
                <div class="label"> 1 Light Sample </div>
                <img class="image-container" src="1_l.png" alt="flatshading">
                </div>
                <div>  
                <div class="label"> 4 Light Samples </div>    
                <img class="image-container" src="4_l.png" alt="phongshading">
                </div>
		<div>  
		<div class="label"> 16 Light Samples </div>    
                <img class="image-container" src="16_l.png" alt="phongshading">
                </div>
		<div>  
		<div class="label"> 64 Light Samples </div>    
                <img class="image-container" src="64_l.png" alt="phongshading">
                </div>
		<div>  
            </div>
	<p class="caption">We see pretty clearly that using more light samples also helps reduce the noise and increase quality to renders. This meakes sense as it is the same idea behind super sampling.
</p>
	
        </section>

        <section>
            <h3>Part 4: Edge Flip</h3>

            <p>The spec provided some really helpful tips that made the implementation of the edge flip more painless. Specifically, stating that we should just reassign all of the pointers for all of structures involved (namely vertices, edges, half-edges, and faces) probably saved a lot of time compared to if we proceeded with the alternative option of trying to find our which structures had a change in pointers and only attempting to modify those. Similarly this advice motivated the idea of first assigning all the involved structures into a variable first and then mechanically reassigning the pointers and swapping things as deemed necessary.</p>

		<p>At a high level the edge flip is changing the vertices that connect a certain edge along the other two points out of the 4 points involved in the two triangles that share the edge. In terms of the actual mechanics of implementing the edge flip one important thing to notice is that no new structures are created to implement this functionality. Therefore, we just need to reassign existing ones. So once we listed out all the structures (vertices a, b, c, d; half-edges ac, cd, db, ba, cb, bc; edges ac, cd, db, ba; faces 1 and 2) on paper we drew out the post-flip state of our 2-triangle geometry and reassigned the appropriate changes. For instance the face of half-edge cd was changed to correspond to the face 1 instead of it being in face 2 as it was previously. Some other half edges also had changes to their next value, specifically the ones on the edge that was flipped. Because of these multiple changes, we just used the post-condition to completely reassign all the values of the pointers. Other structures that we had to modify were the face and vertex pointers. Here the modification was just to make sure that the half-edge they point to actually still corresponds to a half-edge that represents the face or vertex because those could be changed and we could have wrong pointers. The code provides a list of the exact changes but this was the gist of the main challenges of the problem. </p>

	 <div class='im-container'>
                <div>
                <div class="label"> Teapot Before Flips </div>
                <img class="image-container" src="before.png" alt="before">
                </div>
                <div>  
                <div class="label"> Teapot After Flipping One Diagonal Edge </div>    
                <img class="image-container" src="after1.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> Teapot After Flipping Multiple Vertical Edges </div>    
                <img class="image-container" src="after2.png" alt="after2">
                </div>
		<div> 
            </div>
	<p class="caption">Pictures showing the functional edge-flip mechanism.</p>

      <p class="left">Initially while implementing this functionality, I thought for some reason I could reasonably visualize all the pointer changes mentally and proceeded to write the code. Perhaps predictably one could imagine this did not bode well. Then—like I should have done earlier—I pulled out a pencil and paper, drew up the toy example and labeled the key structures both before and after the flip. This made life much easier as the work of the before and after stage was already on paper and I just had to carefully replicate it in code now. Attached for no real purpose is the pen-paper masterpiece.</p>

 <img class="image" src="edgeflipdebug.jpeg" alt="edgeflipdebug">
 <p class="caption">Although the pictures are not too detailed, having a reference to look to helps speed up programming as well as reduces the possibility of errors from my empirical evidence. 
 </p>     

        </section>

        <section>
             <h3>Part 5: Edge Split</h3>
            <p> In a nutshell, in the edge split operation, we attach a new vertex at the midpoint of the edge we are trying to split and then create an edge in the same direction as if we were trying to flip the selected edge. Now the original two triangles have been partitioned into four separate triangles. This means there are 2 new faces (since we have two new triangles), one new vertex (the midpoint), three new edges (across the midpoint), and also six new half-edges. Other than that the rest of the challenge is just reassigning pointers in the old structures to either the new structures that are created through the split or to a new location as a result of the split. Therefore, the edge split follows much of the same basic steps as the flip except we are also adding new structures. Specifically, to tackle the split, like with the flip we first initialized variables for all the structures involved  (vertices a, b, c, d; half-edges ac, cd, db, ba, cb, bc; edges ac, cd, db, ba; faces 1 and 2). Then initialize the new structures that will be introduced post flip (vertex m; faces 3, 4; 6 new half-edges, 3 new edges). Once all of these structures were initialized, the task becomes the same as in the flip-case: correctly populate the field variables of the structures. The most complicated assignments are for the half-edges, but working through a drawn diagram can allow us to systematically label the next half-edge, face, vertex, and twin pointers. Then we simply need to make sure that each of the face, vertex, and edge structures have a correct pointer to the right half-edge. Candidly, there is no real smart algorithm, these pointer adjustments just have to be carefully assigned by just assigning as the drawing of the post-split state indicates. 
</p>
<p>
One thing that helped make this process much smoother was actually giving nice names to the structures instead of the rather unreadable and uninterpretable names we gave them when doing the flip operation implementation. Specifically I labeled the vertices a, b, c, d and edges similarly instead of just using numbers. This way I had a much easier time translating the drawn image state into the actual code that implements that logic. 
</p>
            <div class='im-container'>
                <div>
                <div class="label">Teapot Before Splits </div>
                <img class="image-container" src="tp.png" alt="tp">
                </div>
                <div>  
                <div class="label"> Teapot After One Split </div>    
                <img class="image-container" src="tpflip.png" alt="tpflip">
                </div>
                <div>
                <div class="label"> Teapot After Many Splits </div>
                <img class="image-container" src="tpflips.png" alt="tpflips">
                </div>
            </div>
            <p class="caption">Showing the edge-flip functionality. Note how the last image has multiple splits within one triangle as well.
</p>
           <div class='im-container'>
                <div>
                <div class="label">Teapot Before changes </div>
                <img class="image-container" src="first.png" alt="first">
                </div>
                <div>  
                <div class="label"> Teapot After Applying Initial Splits </div>    
                <img class="image-container" src="second.png" alt="second">
                </div>
                <div>
                <div class="label"> Teapot After Applying Flips to Splits </div>
                <img class="image-container" src="third.png" alt="third">
                </div>
		<div>
                <div class="label"> Teapot After Applying Splits to Flips of Splits </div>
                <img class="image-container" src="last.png" alt="last">
                </div>
            </div>
            <p class="caption">This sequence of photos shows the use of a  functional half-edge split with a functional half-edge flip to create complex transformations.
</p>

		<p>Initially while trying to implement half-edge splitting I had quite unreadable variable names, which did not help in making my job of deciding where pointers lie easier. After trying to get this version working, I decided to give the variables more descriptive names (corresponding with the actual labels I had on my paper diagram). This made it much faster and easier to implement the splitting function. 
</p>
  <img class="image" src="edgesplitdebug.jpeg" alt="edgesplitdebug">
		<p class="caption">Note how again the drawing is not too involved but goes a long way in making programming easier.
</p>
            <h4 class="left">Extra Credit: Boundary Edge Split</h4>
<p class="left"> I implemented the edge-split functionality on boundary edges. Below are the photos of it working</p>
	<div class='im-container'>
                <div>
                <div class="label"> Beetle Car Before </div>
                <img class="image-container" src="carbefore.png" alt="before">
                </div>
                <div>  
                <div class="label"> Applying Boundary Edge Splits with Other Edge Splits </div>    
                <img class="image-container" src="carafter.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> Applying Multiple Boundary Edge Splits </div>    
                <img class="image-container" src="multiple.png" alt="after2">
                </div>
		<div> 
            </div>
	<p class="caption">Pictures showing the functional boundary edge-flip mechanism.</p>
	<p class="left">The implementation was not too bad as I had a lot of pointer manipulation practice from the regular edge-split and edge-flip implementation. The subtly to note is that only one new triangle inside is created and there is one new half-edge that exists on the boundary. After noting those, the steps that proceed are the same as just carefully reassigning pointers in any other case.</p>
        </section>

        <section>
             <h3>Part 6: Loop Subdivision for Mesh Upsampling</h3>

        <p>
	Basically loop subdivision is a method to upsample our given mesh and as mentioned in the overview, relies heavily on our edge-split and flip functionality we worked on. The mechanics of the algorithm to implement mesh upsampling were pretty finely detailed in the spec so it was just a matter of correctly translating those steps into code. Specifically the complexity of trying to upsample lies in two factors: introducing new structures as well as changing the positions of our old structures. However, some of the properties of the new structures depend on the positions and properties of the old ones, which made it important to be careful in the order that changes were made. Specifically, each of the structures had a field that stored the old and new positions as well as a boolean flag which labeled whether or not the structure itself was new or not. Then we computed the new positions of the old vertex (storing into the old vertices’ field) as well as the position of new vertices (which would go inside the edge that would be split to achieve that new vertex). Both the position of the new position of the old vertex and the position of the new vertex were calculated according to the loop division formula that was given. While the subdivision and the edge split and flip of the triangle were described earlier it is easier to calculate out all the positions and then deal with the geometric flipping and splitting that would place the right structures at the right location.	
	</p>

		<p>Next we would split every edge and once we split the edge, utilizing the edge split subroutine. While doing this we propagate over the position of the new vertex from the value that was stored inside the edge structure previously. One thing that we had to be careful about was not splitting newly created edges, but that could be determined easily (if either of the vertices that make up the edge were newly created then the edge is new and we ignore it). 
		</p>

	<p>
	After edge splitting was done, we then flipped edges that connected one new vertex with another old one (also done through using a simple xor on the vertex-is-new flags). And once we determined that an edge needed to be flipped we simply called the flipEdge function we created earlier. Another subtlety here is we had to make sure to only flip new edges since old edges should not be flipped no matter the state of the vertex, which could also be determined by a simple conditional statement on the flags.	
	</p>

	<p>
	Finally we have the mesh ready and just need to make the actual position field for all the vertices the one that is stored in their corresponding newPosition field, completing our upsampling. 	
	</p>

	<p>
	When looking at the behavior of sharp corners and edges under loop subdivision, it is evident these sharp parts of the geometry have a much harsher/larger change at each level of upsample than non sharp corners and edges. Specifically compared to the original geometry, the upsampled version pulls in these corners and edges much more in an effort to make the overall geometry more rounded. Since sharp corners and edges see a much larger inward pull effect than non sharp ones, we disproportionately change the geometry or shrink the sharper areas compared to the non-sharper ones, leading to asymmetry. This also makes sense as in sharper corners, the locations of the other vertices in comparison to ours are much more different, so when we apply the weighted averaging step, we are going to see a large change in the new location of our vertex in the upsampled version, whereas in non-sharp corners and edges our change is not going to be as drastic. In either case our figure does get smaller but that change slows down at larger iterations of upsampling. One reason why we have this disproportionate figure is because the vertices are unevenly distributed. If we were to introduce symmetry on the locations and spacing of the vertex in our figure through splits, we can make our upsampling effect be the same throughout the geometry of our figure, curtailing the asymmetry issue. I show this in the example with fixing the cube.
	</p>
		
<div class='im-container'>
                <div>
                <div class="label"> Level 0 Original Cube </div>
                <img class="image-container" src="lvl0.png" alt="lvl0">
                </div>
                <div>  
                <div class="label"> Level 1 Upscaling </div>    
                <img class="image-container" src="lvl1.png" alt="lzpl">
                </div>
                <div>
                <div class="label"> Level 2 Upscaling </div>
                <img class="image-container" src="lvl2.png" alt="lnpn">
                </div>
                 <div>
                 <div class="label"> Level 3 Upscaling </div>
                <img class="image-container" src="lvl3.png" alt="lnpl">
                </div>
		<div>
                 <div class="label"> Level 4 Upscaling </div>
                <img class="image-container" src="lvl4.png" alt="lnpl">
                </div>
        </div>
        <p class="caption">Comparing the various levels of upscaling it is clear that sharp corners have a much larger inward movement effect compared to non sharp ones when we upscale. Due to this disproportionate movement, we see the eventual asymmetry. 
</p>

		<p>One thing I did to make the cube divide more symmetrically is make each of the faces the sample geometrically by splitting an edge on every face. This way the sharpness of the corners decreases as neighboring vertices are closer to each other and two all vertices will have similar neighbor topologies. One can imagine that increasing the number of triangles that make up our cube symmetrically will do an even better job at maintaining symmetry through various upsampling levels.
</p>
		<div class='im-container'>
                <div>
                <div class="label"> Level 0 Modified Cube </div>
                <img class="image-container" src="c0.png" alt="lvl0">
                </div>
                <div>  
                <div class="label"> Level 1 Upscaling </div>    
                <img class="image-container" src="c1.png" alt="lzpl">
                </div>
                <div>
                <div class="label"> Level 2 Upscaling </div>
                <img class="image-container" src="c2.png" alt="lnpn">
                </div>
                 <div>
                 <div class="label"> Level 3 Upscaling </div>
                <img class="image-container" src="c3.png" alt="lnpl">
                </div>
		<div>
                 <div class="label"> Level 4 Upscaling </div>
                <img class="image-container" src="c4.png" alt="lnpl">
                </div>
        </div>
		 <p class="caption">Notice how my changes helped improve upscale symmetry.
</p>
		<h4 class="left">Extra Credit: Support meshes with boundary</h4>
		<p class="left">In implementing supporting upscaling on the boundary at the boundary, I first looked up the rules for assigning new vertex (on the edge structure) and old vertex positions (on the vertex structure itself). This was found in the lecture slides on the lecture for Mesh Processing. Specifically the new position for old vertices corresponds to ⅛ * (sum of adjacent boundary vertex on boundary edges) + ¾ * (current vertex position). Similarly the update rule for a new vertex position is quite simple ½ * (sum of vertex positions that make up the edge). Once I knew these rules, much of the skeleton to add them in our upscale method already existed. Specifically I only had to add conditionals in the existing for loops that calculated new positions for old vertices and also the new positions on edges so they can be set on new vertices. The conditional corresponds to whether or not the edge or vertex I am dealing with is a boundary edge. If it was then we apply our new special logic. The application of the logic was quite straightforward as we just did simple pointer traversal and summing like previously implemented. With these changes, support for boundary upscaling was implemented. 
</p>
			<div class='im-container'>
                <div>
                <div class="label"> Level 0 Beetle Car </div>
                <img class="image-container" src="normalbeetle.png" alt="lvl0">
                </div>
                <div>  
                <div class="label"> Level 1 Upscaling on Boundary </div>    
                <img class="image-container" src="lvl1boundary.png" alt="boundary">
                </div>
                <div>
                <div class="label"> Level 2 Upscaling on Boundary </div>
                <img class="image-container" src="lvl2boundary.png" alt="boundary2">
                </div>		
        </section>
			<p class="caption">Notice how boundary meshes are also being upscaled.
</p>

<!-- 
			<table>
                <thead>
                    <tr>
                        <th>Column 1</th>
                        <th>Column 2</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Data 1</td>
                        <td>Data 2</td>
                    </tr>
                    <tr>
                        <td>Data 3</td>
                        <td>Data 4</td>
                    </tr>
                </tbody>
            </table> -->
        
        <!-- Add more sections as needed -->
        
    </div>
</body>
</html>
