<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 184 Homework 2, 
Aishik Bhattacharyya, Tanush Talati</title>
    <style>
        .caption {
            text-align: center;
            font-style: italic;
        }
        .im-container {
            display: flex; 
            justify-content: space-between; 
            margin-bottom: -10px;
            flex-wrap: wrap;
            text-align: center;
        }
        .label {
            text-align: center;
        }
        .image-container {
            width: 70%;
        }
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .image {
            width: 100%;
            margin-bottom: -10px;
        }
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            position: fixed;
            bottom: 0;
            width: 100%;
        }
        h1, h2, h3, h4, h5, h6 {
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <header>
        <h1>CS 184 Homework 2: Curves, Surfaces, and Meshes Writeup</h1>
        <h2> 
Aishik Bhattacharyya, Tanush Talati </h2>
    </header>
    
    <div class="container">
        <h2>Overview</h2>

	<p>
	
	</p>
	    
        <section>
            <h3>Task 1: Bezier Curves with 1D de Casteljau Subdivision </h3>
            
            
            <p>We use de Casteljau’s algorithm to transform n control points to n-1 control points in the next subdivision level. These n control points define the Bezier curve we want to analyze. We are also given a parameter t that defines where in the Bezier curve where we want to interpolate along the curve. For every nth point, we also take the (n-1)th point to interpolate and compute the new control point. To interpolate, we use the given function from the spec and run it n-1 times.  This evaluates one step of de Casteljau’s algorithm. The algorithm is continued until the appropriate level of detail is acquired. In the image below, we can see that in each subdivision there is one less point constructing this graph until there is only one point left. Clearly, these steps help determine the curvature of the Bezier curve and the final output is a legitimate output of applying the algorithm.            
            </p>
		<img class="image" src="task1.PNG" alt="Task1 6-point Test">
		<img class="image" src="task11.PNG" alt="Task1 Modified 6-point Test">
        </section>
        
        <section>
            <h3>Task 2: Bezier Curves with Seperable 1D de Casteljau </h3>
            <p> De Casteljau’s algorithm extends to Bezier surfaces by using our Code in task 1 to apply on to the u and v directions. Given a set of control points that define a Bezier curve, we loop through every point and calculate a new set of points along the u direction for the given parameter t. For this, we created a new function which fully evaluates de Casteljau’s algorithm for a vector of points at parameter t.  Once we calculate this new set of points, seen as a column of u’s for every row, we run them through the same function we did for the u direction to get one step of the Bezier surface evaluation process. These steps are repeated until the Bezir surface is fully divided into. To visualize our image, we connect the final points and produce an output like below.
	    </p>
            <div class='im-container'>
                <div>
                <div class="label"> Task2 Teapot Test </div>
                <img class="image-container" src="task2.PNG" alt="task2_1">
                </div>
                <div> 
            </div>

            <h4>Extra Credit</h4>
            <p>The alternative sampling method I implemented was jittered sampling. Essentially jittered sampling is the idea that instead of sampling at deterministic points for our supersampling, we sample uniformly within the sub pixels that are of super sampling. Otherwise, the rest of the supersampling machinery is the same. At lower super sampling rates we do see more jaggedness and the jaggedness is quite random. But at higher super-sampling rates the jittered sampling method does really help reduce aliasing. However the biggest issue with this sampling method is if the border of a triangle goes through the center of the triangle, we see random white spots since there is a good chance that none of the sampled pixels land inside the triangle or ½ land inside and ½ outside leading to a discoloration. Other than that whenever we have sharp frequency changes it seems that with lesser super samples, we are able to better anti-alias it with jittered sampling.
</p>     
            <div class='im-container'>
                <div>
                <div class="label"> Jitter Sampling rate = 1 </div>
                <img class="image-container" src="jitter1.png" alt="task2_1">
                </div>
                <div>  
                <div class="label"> Jitter Sampling rate = 9 </div>    
                <img class="image-container" src="jitter9.png" alt="task2_4">
                </div>
            </div>

            <p class="caption">Notice how the jitter sampling tends to perform much worse at lower sampling rates but also does a good job of capturing high frequencies that a deterministic sampling method would not. It also takes less of a super sampling frequency to get the same performance as a deterministic sampling method (9 vs 16 we saw earlier)</p>
            
        </section>

        <section>
            <h3>Task 3: Area-Weighted Vertex Normals Surfaces with S</h3>
<p> To implement area-weighted vertex normals, we want to traverse the geometry until we come back to our starting half edge pointer. Throughout our iteration, we check the face associated with our starting half edge pointer and ensure that it is not a boundary. From the current edge, we calculate the vertex it’s on and the next two vertices in its path. This gives us 3 points in a triangle. By calculating the cross product of the line vectors between them, we can calculate the triangle face area by normalizing the stated cross product and dividing by 2. The formula behind this is basic geometry. Then we keep track of a variable that contains the weighted sum of face normals, calculated by taking the product of the calculated area and the normal of the face of the current pointer. Finally at the end, we normalize the final weighted sum.
</p>
            <img class="image" src="task3.PNG" alt="Test3 Teapot Shading w/o vertex normals">
            <img class="image" src="task31.PNG" alt="Test3 Teapot Shading w/ vertex normals">

            <h4>Extra Credit</h4>
            <p>For the GUI I decided to add a feature that could rotate the viewport. Pressing the button R would rotate one degree clockwise whereas press T would rotate it one degree counterclockwise. The way I implemented this was first by adding a listener to the T and R keys in the keyboard_event function. This would change a new variable I made for the class called degree_rot by one degree in the respective direction. Then I also changed the redraw method to left-multiply the ndc_to_screen matrix with the rotation degree that takes the parameter of degree_rot. This would achieve the rotation if I called redraw every time the key_listen event was triggered.</p>

            <img class="image" src="rotcw.png" alt="flying_robot">
            <img class="image" src="rotccw.png" alt="flying_robot">

            <p class="caption"> Visuals of the rotation feature. It is capped at 15 degrees both ways to prevent bugs. </p>
            
        </section>

        <section>
            <h3>Task 4: Barycentric Coordinates</h3>

            <p>Essentially barycentric coordinates are another coordinate space for triangles (and multidimensional triangle shapes). It allows us to represent points on a coordinate plane with respect to the vertices of the triangle. These vertices are located at the barycentric coordinates of (1, 0, 0), (0, 1, 0), and (0, 0, 1) respectively. Where the specific indices represent alpha, beta, and gamma. Among all the special properties of these coordinates, one of particular interest is the fact that if alpha + beta + gamma = 1 and are all less than 1, the coordinates represent a point inside of the triangle. In this constrained view, we can think of the barycentric coordinates for a particular point as a weighted sum of all the vertices of the triangle. Indeed, the point (⅓, ⅓, ⅓) actually represents the centroid of the triangle and in the weighted sum view we just took the average of all the points. I described the process of finding the barycentric coordinate of a particular point in the overview section of this writeup. However, along with the fact that points can be viewed as a weighted sum of the vertex, if the vertices are associated with certain properties we can interpolate the property value at a sample point inside the triangle by weighting the sum with the barycentric coordinates. This is done for color interpolation and sampling when each vertex of the triangle is associated with a color and we need to assign a color to the rasterized points inside of the triangle. The picture below better showcases this phenomenon. The top vertex is associated with red, right with blue, and left with green. Notice how points close to any of these vertices contain a color close to that of the vertex but as we move toward the center or along each line there is a mixture. The weights of the mixture are the ones represented by the barycentric coordinates.  </p>

<img class="image" src="triangle.png" alt="barycentric interpolation">
 <p class="caption">Triangle with colors inside interpolated using barycentric weights can also be intrepreted as the color represents the barycentric coordinate at a point.</p>

<img class="image" src="colorwheel.png" alt="colorwheel interpolation">
 <p class="caption">Color wheel with the colors rasterized with the help of barycenter coordinates</p>


        </section>

        <section>
             <h3>Task 5: Pixel Sampling for Texture Mapping</h3>
            <p> Previously colors were being assigned to the points we were trying to rasterize using barycentric interpolation and attaching color values to the specific points in our geometry. However with texture mapping each vertex in our triangle is associated with a corresponding texture coordinate. This texture coordinate represents another space that we sample from to determine the color of the particular vertex. This means that when we are tasked with rasterizing our triangle the samples we take inside the triangle will be assigned to a corresponding texture coordinate for the sample. This sampling will necessarily be in the continuous domain, however we only have texture coordinate to color mapping for lattice points in the texture coordinate space. Therefore to determine the color of the sample, we need to also sample in the texture space. The two methods of sampling that were implemented were nearest neighbor sampling, which would just return the texture color of the closest lattice texture point to the sample texture coordinate; and bilinear sampling, which would find the 4 closest lattice points and then depending on the distance from each of these point, the final color would be the weighted average of all the colors at the 4 points. In this manner we can view bilinear sampling as taking more samples of the texture space than does the simple nearest neighbor. The process of finding the corresponding texture coordinate for each sample coordinate is described further in the overview section.
</p>
            <div class='im-container'>
                <div>
                <div class="label"> NN Interpolation, Sampling Rate = 1 </div>
                <img class="image-container" src="nearest_1.png" alt="nearest_1">
                </div>
                <div>  
                <div class="label"> Bilinear Interpolation, Sampling Rate = 16 </div>    
                <img class="image-container" src="bilinear_1.png" alt="bilinear_1">
                </div>
                <div>
                <div class="label"> NN Interpolation, Sampling Rate = 16 </div>
                <img class="image-container" src="nearest_16.png" alt="nearest_16">
                </div>
                <div>
                <div class="label"> Bilinear Interpolation, Sampling Rate = 16 </div>
                <img class="image-container" src="bilinear_16.png" alt="bilinear_16">
                </div>
            </div>
            <p class="caption">Comparing bilinear vs nearest neighbor pixel sampling on various sampling rates.
</p>
           <p> Examining the photos it is pretty clear that bilinear interpolation does a better job at antialiasing compared to nearest neighbor interpolation when the sampling rate is the same. There is also more anti-aliasing when the sampling rate is higher (which is expected behavior since we increase our sampling rate) so the discrepancy between bilinear and nearest neighbor at higher sampling rate is lesser. The reason bilinear does a better job is because in bilinear we sample the closest 4 texel colors to assign the color of the pixel compared to just sampling one, which is inherently a higher sampling rate, leading to more anti-aliasing. Extending from that observation, we will notice larger differences between the two sampling methods when the frequency in the texture space is very high so when the signal changes quickly our sampled output will be less jagged than if we used nearest neighbor as there are more samples we are taking into account to determine the color at out pixel. </p>
            
            
        </section>

        <section>
             <h3>Task 6: Level Sampling with Mipmaps</h3>

            <p>The main issue with solely pixel sampling is the fact that as we move a singular sample inside the image space this could correspond to very different footprint movements inside of the texture space. Namely, we could have a very extreme case of aliasing if the movement across the x or y inside of the pixel space corresponded to a very large change inside of the texture space, leading to aliasing when we are trying to sample the texture space in order to set a color for the sample in our image space. This is why we introduce level sampling; we basically calculate for a given image sample, the footprint the sample has on the corresponding texture space. If the footprint is very large, then we sample for a representation of the texture that has filtered out high frequencies in order to introduce antialiasing. This basic idea led to the creation of a data structure called the mipmap. Essentially the mipmap stores all the different levels of a textured image. With each subsequent level, the image of the mipmap level  contains increasingly filtered versions of the original texture map (we are filtering more and more frequencies of the texture map at higher mipmap levels). Now for each screen space point we sample, we calculate the footprint on the texture map (which is the larger of distance in either the U/V direction we move when we move in X/Y). And the level which we sample at is proportional logarithmically to this distance (which makes sense since each subsequent level halves the number of pixels in each direction, implying the same proportionality). There is a possibility that the logarithm returns a number less than 0 or more than the levels in our mipmap. In that case we sanitize the output and simply query the mipmap at level 0 or the highest value respectively. With the level chosen for each pixel, the mechanism for sampling the map at each level from the previous part still applies, giving us a powerful two-phase texture sampling, leading to extremely nice rendering. Furthermore since the level returned is a continuous value, the level itself can be determined through a bilinear interpolation of the mixture of the color returned from the two closest levels to not have drastic color changes, or we can just use nearest neighbors. Clearly, implementing level sampling along with pixel sampling greatly increased the number of options available to render an image. </p>

            <p>In the implementation of the project, we already were given the mipmap data structure. Largely the work corresponded to calculating the footprint our sample had on the texture map. This was done by moving in the x and y directions in the image space by one unit and then seeing the corresponding coordinate that would give us in the texture space. The coordinate conversions were done similarly to how they were performed in the previous step. We took the larger of the changes in coordinates in the texture space as our heuristic to calculate the level of the mipmap we would sample from. And as mentioned previously since the logarithm is a continuous function, we had a few options on how we would interpolate level sampling (nearest vs bilinear). Once those decisions were made we already had the machinery to conduct pixel sampling at the level chosen by our level sampling scheme. </p>

            <p>In the next paragraphs I will discuss the tradeoffs of pixel sampling, level sampling, and samples per pixel.
</p>
            <p>With increasing the number of samples per pixel our memory overload increases by a factor of the sampling rate since we will need to store data for each of those super samples for every pixel. We will also need to iterate through all of those new samples, leading to an increase in time complexity by the same factor. While we get higher memory and time utilization, it does increase our anti-aliasing power since more samples per pixel effectively increases our sampling frequency and hence our nyquist frequency, allowing us to properly represent increasing frequencies.
</p>
            <p>When level sampling through a mipmap our memory utilization increases by a factor of 4/3 due to the geometric decrease in the number of texels stored at every increased level. On the other hand our speed should not change significantly. While we do have to make extra calculations such as calculating the level to index in the mipmap and the distance footprint in the texture space, these are just a few extra computations that should not have too large of an impact. We also do get the benefit of added antialiasing because depending on the variability of the footprint on the texture space we sample from the correctly anti-aliased image, giving good anti-aliasing for the overall picture.</p>

            <p>The complexity of pixel sampling really depends on the scheme being used. However regardless of whether we use nearest neighbor or bilinear pixel sampling, the memory should not change as we are just sampling for the texture map that already exists in memory and are not incurring any additional overhead. If we decide to use bilinear vs nearest neighbor, the time to complete will be a bit longer since bilinear requires us to sample the 4 closest texels as well as compute 3 linear interpolations and nearest neighbor only 1 sample. Due to the additional amount of sampling in bilinear interpolation, we also enjoy the added benefit of more antialiasing power. 
</p>

     <div class='im-container'>
                <div>
                <div class="label"> Level 0, Point NN </div>
                <img class="image-container" src="lzpn.png" alt="lzpn">
                </div>
                <div>  
                <div class="label"> Level 0, Point Bilinear </div>    
                <img class="image-container" src="lzpl.png" alt="lzpl">
                </div>
                <div>
                <div class="label"> Level Nearest, Point NN </div>
                <img class="image-container" src="lnpn.png" alt="lnpn">
                </div>
                 <div>
                 <div class="label"> Level Nearest, Point Bilinear </div>
                <img class="image-container" src="lnpl.png" alt="lnpl">
                </div>
        </div>
        <p class="caption">Comparing various level and point sampling combinations. Having a high complexity of both seems to smooth out edges the best, leading to higher anti-aliasing power.
</p>
            
        </section>
        
        
        <!-- Add more sections as needed -->
        
    </div>
</body>
</html>
