<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 184 Homework 1, Tanush Talati</title>
    <style>
        .caption {
            text-align: center;
            font-style: italic;
        }
        .im-container {
            display: flex; 
            justify-content: space-between; 
            margin-bottom: -10px;
            flex-wrap: wrap;
            text-align: center;
        }
        .label {
            text-align: center;
        }
        .image-container {
            width: 70%;
        }
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .image {
            width: 100%;
            margin-bottom: -10px;
        }
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            position: fixed;
            bottom: 0;
            width: 100%;
        }
        h1, h2, h3, h4, h5, h6 {
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <header>
        <h1>CS 184 Homework 1: Rasterizer Writeup</h1>
        <h2> Tanush Talati </h2>
    </header>
    
    <div class="container">
        <h2>Overview</h2>
        <p>In this homework I worked on implementing many key components of a rasterizer. </p> 
        
<p> 
First, I implemented rasterizing triangle polygons onto the screen space. The main challenge here was determining whether or not a screen pixel exists inside of the triangle of interest. If it does, then the sample buffer, which contains the pixels (or the set of fragments that will then make up some pixels in the case of supersampling) will be set to the same color as the triangle that is being rasterized. In determining the inside-triangle test, I had to figure out the exact vector-algebra machinery that would output whether a point was inside a triangle given by a set of three vertices. This was probably the most challenging part of the first task as it tested my knowledge of tangent lines, planes, and cross products. The other part I had to figure out was converting the set of given points into a consistent winding order. I decided this would be counterclockwise. In order to do this I had to rely on vector-algebra again, specifically cross-products (which would be positive if the points were in counterclockwise winding order). 
</p>

<p>
Second, I extended the functionality of the simple triangle rasterizer by implementing super-sampling as a means to introduce anti-aliasing since super-sampling effectively increases sampling frequency. The changes made to the existing rasterize_triangle method were pretty simple; I just had to carefully calculate the position of the super-samples within a pixel using the sampling-rate of our RasterizeImp (done through some drawing and deriving the correct equations). The most challenging part was to edit the rest of the codebase to support sampling. Namely, dynamically changing the size of the sample_buffer depending on the sampling rate as well as resolving the final color to the frame buffer by assigning the color rasterized as a mixture of the samples within a pixel. These components touched a lot of different methods within the codebase that had to be tracked down.
</p>

<p>The next task was to implement transforms. This was quite straightforward, the transformation matrices for rotations, translations, and scaling were already derived before and had to just be written out in code in their corresponding functions.
</p>
        
<p>The fourth task was an extension on the rasterization of triangles. This time each vertex had a different color assigned to it, so we were no longer dealing with a single colored polygon. That meant that the sampled points within the triangle that were going to finally go into the frame buffer would need an interpolated mixture of the various colors the triangle was made up of. To implement this, barycentric coordinates were heavily utilized. Namely, barycentric coordinates, made up of the variables alpha, beta, and gamma can be thought of as assigning weights to each vertex of the triangle, having the property that each position relative to the triangle can be represented in barycentric coordinate space instead of x,y,z space. This view gives the nice property of thinking about the barycentric coordinates of points inside the triangle as encoding the weighted sum of each of its vertices. This implied that finding the barycentric coordinate of the point within the triangle being sampled would also allow us to find the weights with which we mix the colors at each of the vertices to assign the final color of the point. It also translates easily to supersampling with no changes necessary. The hardest part was simply determining the barycentric coordinates, which was done by calculating the equation for each of the lines of the triangle and then comparing the ratio of the value evaluated at the point being sampled to the point across from the line (which would be 1). It turns out that instead of finding the equation equating the dot product of the normal of the line with the line anchored at the beginning point of the line in the triangle to the point being sampled would amount to the values mentioned earlier.</p>

<p>
The fifth task had me implement pixel texture sampling. Basically now the coordinates of each of the vertices in the triangles were associated with a texture-sample coordinate, which we would use to determine the color of the various samples of the triangle instead of relying on a color value like we did before. This goes along with the idea of “sampling twice” as mentioned in lecture since we sample points inside the triangle to eventually rasterize and we also sample those corresponding points on the texture space to assign each of the samples a color which is also a sample but from the texture maps. So essentially we needed to find the corresponding texture space coordinate for each sample. This was done through barycentric coordinates since those encode relative positions, so finding the barycentric coordinates and then multiplying those by the coordinates in texture space of the triangle would give the underlying texture coordinate of each point being sampled. Now the texture map is a discrete valued data structure that returns color for each coordinate, however our coordinates are continuous, so essentially to determine a color we need to sample again. Two sampling schemes were implemented, one that just returned the color of the nearest discrete coordinate to the query. And another one that implemented bilinear interpolation, which essentially gave the weighted color average of the closest 4 texture points to the query, weighted by relative distances to those points from the query point. In this way we were able to finally assign a color to the screen sampled coordinate utilizing our texture map.
</p>

<p>The last task extended the capabilities of our texture mapping system by introducing level sampling. Essentially the issue with 
solely using pixel sampling is aliasing because movement in screen space could have very different footprints on the texture 
space; for instance moving by one unit in either the x or the y direction could correspond to multiple units of movement in the 
texture space, leading to aliasing since our samples of the texture space are changing with extremely high frequency and our 
overall sampling frequency is too low. This motivates the idea that when the screen space movement footprint is too high for 
sample space, we want to sample the color that we are assigning from a lower frequency texture map to help anti-alias. This is 
where level-sampling and mipmaps come into play. Basically mipmaps are the data structure that powers level sampling. Each 
increasing level of the mipmap contains increasingly filtered versions of the original texture map (we are filtering more and 
more frequencies of the texture map at higher mipmap levels). Now for each screen space point we sample, we calculate the 
footprint on the texture map (which is the larger of distance in either the U/V direction we move when we move in X/Y). And the 
level which we sample at is proportional logarithmically to this distance (which makes sense since each subsequent level halves 
the number of pixels in each direction, implying the same proportionality). There is a possibility that the logarithm returns a 
number less than 0 or more than the levels in our mipmap. In that case we sanitize the output and simply query the mipmap at 
level 0 or the highest value respectively. With the level chosen for each pixel, the mechanism for sampling the map at each 
level from the previous part still applies, giving us a powerful two-phase texture sampling, leading to extremely nice 
rendering. Furthermore since the level returned is a continuous value, the level itself can be determined through a bilinear 
interpolation of the mixture of the color returned from the two closest levels to not have drastic color changes, or we can just 
use nearest neighbors. Clearly, implementing level sampling along with pixel sampling greatly increased the number of options 
available to render an image. 
</p>
        
        </p>
        <section>
            <h3>Task 1: Drawing Single-Color Triangles</h3>
            
            
            <p>We use de Casteljau’s algorithm to transform n control points to n-1 control points in the next subdivision level. These n control points define the Bezier curve we want to analyze. We are also given a parameter t that defines where in the Bezier curve where we want to interpolate along the curve. For every nth point, we also take the (n-1)th point to interpolate and compute the new control point. To interpolate, we use the given function from the spec and run it n-1 times.  This evaluates one step of de Casteljau’s algorithm. The algorithm is continued until the appropriate level of detail is acquired. In the image below, we can see that in each subdivision there is one less point constructing this graph until there is only one point left. Clearly, these steps help determine the curvature of the Bezier curve and the final output is a legitimate output of applying the algorithm.            
            </p>
		<img class="image" src="test1.PNG" alt="Task1 6-point Test">
		<img class="image" src="test11.PNG" alt="Task1 Modified 6-point Test">
        </section>
        
        <section>
            <h3>Task 2: Antialiasing by Supersampling</h3>
            <p> De Casteljau’s algorithm extends to Bezier surfaces by using our Code in task 1 to apply on to the u and v directions. Given a set of control points that define a Bezier curve, we loop through every point and calculate a new set of points along the u direction for the given parameter t. For this, we created a new function which fully evaluates de Casteljau’s algorithm for a vector of points at parameter t.  Once we calculate this new set of points, seen as a column of u’s for every row, we run them through the same function we did for the u direction to get one step of the Bezier surface evaluation process. These steps are repeated until the Bezir surface is fully divided into. To visualize our image, we connect the final points and produce an output like below.
	    </p>
            <div class='im-container'>
                <div>
                <div class="label"> Task2 Teapot Test </div>
                <img class="image-container" src="task2.PNG" alt="task2_1">
                </div>
                <div> 
            </div>

            <h4>Extra Credit</h4>
            <p>The alternative sampling method I implemented was jittered sampling. Essentially jittered sampling is the idea that instead of sampling at deterministic points for our supersampling, we sample uniformly within the sub pixels that are of super sampling. Otherwise, the rest of the supersampling machinery is the same. At lower super sampling rates we do see more jaggedness and the jaggedness is quite random. But at higher super-sampling rates the jittered sampling method does really help reduce aliasing. However the biggest issue with this sampling method is if the border of a triangle goes through the center of the triangle, we see random white spots since there is a good chance that none of the sampled pixels land inside the triangle or ½ land inside and ½ outside leading to a discoloration. Other than that whenever we have sharp frequency changes it seems that with lesser super samples, we are able to better anti-alias it with jittered sampling.
</p>     
            <div class='im-container'>
                <div>
                <div class="label"> Jitter Sampling rate = 1 </div>
                <img class="image-container" src="jitter1.png" alt="task2_1">
                </div>
                <div>  
                <div class="label"> Jitter Sampling rate = 9 </div>    
                <img class="image-container" src="jitter9.png" alt="task2_4">
                </div>
            </div>

            <p class="caption">Notice how the jitter sampling tends to perform much worse at lower sampling rates but also does a good job of capturing high frequencies that a deterministic sampling method would not. It also takes less of a super sampling frequency to get the same performance as a deterministic sampling method (9 vs 16 we saw earlier)</p>
            
        </section>

        <section>
            <h3>Task 3: Transforms</h3>

            <img class="image" src="flying_robot.png" alt="flying_robot">
            <p class="caption">The attempt was to make the cube man look as if it was flying at an angle up in the sky flat. This is why it was rotated with its hands up in the air and its legs slightly bent. </p>

            <h4>Extra Credit</h4>
            <p>For the GUI I decided to add a feature that could rotate the viewport. Pressing the button R would rotate one degree clockwise whereas press T would rotate it one degree counterclockwise. The way I implemented this was first by adding a listener to the T and R keys in the keyboard_event function. This would change a new variable I made for the class called degree_rot by one degree in the respective direction. Then I also changed the redraw method to left-multiply the ndc_to_screen matrix with the rotation degree that takes the parameter of degree_rot. This would achieve the rotation if I called redraw every time the key_listen event was triggered.</p>

            <img class="image" src="rotcw.png" alt="flying_robot">
            <img class="image" src="rotccw.png" alt="flying_robot">

            <p class="caption"> Visuals of the rotation feature. It is capped at 15 degrees both ways to prevent bugs. </p>
            
        </section>

        <section>
            <h3>Task 4: Barycentric Coordinates</h3>

            <p>Essentially barycentric coordinates are another coordinate space for triangles (and multidimensional triangle shapes). It allows us to represent points on a coordinate plane with respect to the vertices of the triangle. These vertices are located at the barycentric coordinates of (1, 0, 0), (0, 1, 0), and (0, 0, 1) respectively. Where the specific indices represent alpha, beta, and gamma. Among all the special properties of these coordinates, one of particular interest is the fact that if alpha + beta + gamma = 1 and are all less than 1, the coordinates represent a point inside of the triangle. In this constrained view, we can think of the barycentric coordinates for a particular point as a weighted sum of all the vertices of the triangle. Indeed, the point (⅓, ⅓, ⅓) actually represents the centroid of the triangle and in the weighted sum view we just took the average of all the points. I described the process of finding the barycentric coordinate of a particular point in the overview section of this writeup. However, along with the fact that points can be viewed as a weighted sum of the vertex, if the vertices are associated with certain properties we can interpolate the property value at a sample point inside the triangle by weighting the sum with the barycentric coordinates. This is done for color interpolation and sampling when each vertex of the triangle is associated with a color and we need to assign a color to the rasterized points inside of the triangle. The picture below better showcases this phenomenon. The top vertex is associated with red, right with blue, and left with green. Notice how points close to any of these vertices contain a color close to that of the vertex but as we move toward the center or along each line there is a mixture. The weights of the mixture are the ones represented by the barycentric coordinates.  </p>

<img class="image" src="triangle.png" alt="barycentric interpolation">
 <p class="caption">Triangle with colors inside interpolated using barycentric weights can also be intrepreted as the color represents the barycentric coordinate at a point.</p>

<img class="image" src="colorwheel.png" alt="colorwheel interpolation">
 <p class="caption">Color wheel with the colors rasterized with the help of barycenter coordinates</p>


        </section>

        <section>
             <h3>Task 5: Pixel Sampling for Texture Mapping</h3>
            <p> Previously colors were being assigned to the points we were trying to rasterize using barycentric interpolation and attaching color values to the specific points in our geometry. However with texture mapping each vertex in our triangle is associated with a corresponding texture coordinate. This texture coordinate represents another space that we sample from to determine the color of the particular vertex. This means that when we are tasked with rasterizing our triangle the samples we take inside the triangle will be assigned to a corresponding texture coordinate for the sample. This sampling will necessarily be in the continuous domain, however we only have texture coordinate to color mapping for lattice points in the texture coordinate space. Therefore to determine the color of the sample, we need to also sample in the texture space. The two methods of sampling that were implemented were nearest neighbor sampling, which would just return the texture color of the closest lattice texture point to the sample texture coordinate; and bilinear sampling, which would find the 4 closest lattice points and then depending on the distance from each of these point, the final color would be the weighted average of all the colors at the 4 points. In this manner we can view bilinear sampling as taking more samples of the texture space than does the simple nearest neighbor. The process of finding the corresponding texture coordinate for each sample coordinate is described further in the overview section.
</p>
            <div class='im-container'>
                <div>
                <div class="label"> NN Interpolation, Sampling Rate = 1 </div>
                <img class="image-container" src="nearest_1.png" alt="nearest_1">
                </div>
                <div>  
                <div class="label"> Bilinear Interpolation, Sampling Rate = 16 </div>    
                <img class="image-container" src="bilinear_1.png" alt="bilinear_1">
                </div>
                <div>
                <div class="label"> NN Interpolation, Sampling Rate = 16 </div>
                <img class="image-container" src="nearest_16.png" alt="nearest_16">
                </div>
                <div>
                <div class="label"> Bilinear Interpolation, Sampling Rate = 16 </div>
                <img class="image-container" src="bilinear_16.png" alt="bilinear_16">
                </div>
            </div>
            <p class="caption">Comparing bilinear vs nearest neighbor pixel sampling on various sampling rates.
</p>
           <p> Examining the photos it is pretty clear that bilinear interpolation does a better job at antialiasing compared to nearest neighbor interpolation when the sampling rate is the same. There is also more anti-aliasing when the sampling rate is higher (which is expected behavior since we increase our sampling rate) so the discrepancy between bilinear and nearest neighbor at higher sampling rate is lesser. The reason bilinear does a better job is because in bilinear we sample the closest 4 texel colors to assign the color of the pixel compared to just sampling one, which is inherently a higher sampling rate, leading to more anti-aliasing. Extending from that observation, we will notice larger differences between the two sampling methods when the frequency in the texture space is very high so when the signal changes quickly our sampled output will be less jagged than if we used nearest neighbor as there are more samples we are taking into account to determine the color at out pixel. </p>
            
            
        </section>

        <section>
             <h3>Task 6: Level Sampling with Mipmaps</h3>

            <p>The main issue with solely pixel sampling is the fact that as we move a singular sample inside the image space this could correspond to very different footprint movements inside of the texture space. Namely, we could have a very extreme case of aliasing if the movement across the x or y inside of the pixel space corresponded to a very large change inside of the texture space, leading to aliasing when we are trying to sample the texture space in order to set a color for the sample in our image space. This is why we introduce level sampling; we basically calculate for a given image sample, the footprint the sample has on the corresponding texture space. If the footprint is very large, then we sample for a representation of the texture that has filtered out high frequencies in order to introduce antialiasing. This basic idea led to the creation of a data structure called the mipmap. Essentially the mipmap stores all the different levels of a textured image. With each subsequent level, the image of the mipmap level  contains increasingly filtered versions of the original texture map (we are filtering more and more frequencies of the texture map at higher mipmap levels). Now for each screen space point we sample, we calculate the footprint on the texture map (which is the larger of distance in either the U/V direction we move when we move in X/Y). And the level which we sample at is proportional logarithmically to this distance (which makes sense since each subsequent level halves the number of pixels in each direction, implying the same proportionality). There is a possibility that the logarithm returns a number less than 0 or more than the levels in our mipmap. In that case we sanitize the output and simply query the mipmap at level 0 or the highest value respectively. With the level chosen for each pixel, the mechanism for sampling the map at each level from the previous part still applies, giving us a powerful two-phase texture sampling, leading to extremely nice rendering. Furthermore since the level returned is a continuous value, the level itself can be determined through a bilinear interpolation of the mixture of the color returned from the two closest levels to not have drastic color changes, or we can just use nearest neighbors. Clearly, implementing level sampling along with pixel sampling greatly increased the number of options available to render an image. </p>

            <p>In the implementation of the project, we already were given the mipmap data structure. Largely the work corresponded to calculating the footprint our sample had on the texture map. This was done by moving in the x and y directions in the image space by one unit and then seeing the corresponding coordinate that would give us in the texture space. The coordinate conversions were done similarly to how they were performed in the previous step. We took the larger of the changes in coordinates in the texture space as our heuristic to calculate the level of the mipmap we would sample from. And as mentioned previously since the logarithm is a continuous function, we had a few options on how we would interpolate level sampling (nearest vs bilinear). Once those decisions were made we already had the machinery to conduct pixel sampling at the level chosen by our level sampling scheme. </p>

            <p>In the next paragraphs I will discuss the tradeoffs of pixel sampling, level sampling, and samples per pixel.
</p>
            <p>With increasing the number of samples per pixel our memory overload increases by a factor of the sampling rate since we will need to store data for each of those super samples for every pixel. We will also need to iterate through all of those new samples, leading to an increase in time complexity by the same factor. While we get higher memory and time utilization, it does increase our anti-aliasing power since more samples per pixel effectively increases our sampling frequency and hence our nyquist frequency, allowing us to properly represent increasing frequencies.
</p>
            <p>When level sampling through a mipmap our memory utilization increases by a factor of 4/3 due to the geometric decrease in the number of texels stored at every increased level. On the other hand our speed should not change significantly. While we do have to make extra calculations such as calculating the level to index in the mipmap and the distance footprint in the texture space, these are just a few extra computations that should not have too large of an impact. We also do get the benefit of added antialiasing because depending on the variability of the footprint on the texture space we sample from the correctly anti-aliased image, giving good anti-aliasing for the overall picture.</p>

            <p>The complexity of pixel sampling really depends on the scheme being used. However regardless of whether we use nearest neighbor or bilinear pixel sampling, the memory should not change as we are just sampling for the texture map that already exists in memory and are not incurring any additional overhead. If we decide to use bilinear vs nearest neighbor, the time to complete will be a bit longer since bilinear requires us to sample the 4 closest texels as well as compute 3 linear interpolations and nearest neighbor only 1 sample. Due to the additional amount of sampling in bilinear interpolation, we also enjoy the added benefit of more antialiasing power. 
</p>

     <div class='im-container'>
                <div>
                <div class="label"> Level 0, Point NN </div>
                <img class="image-container" src="lzpn.png" alt="lzpn">
                </div>
                <div>  
                <div class="label"> Level 0, Point Bilinear </div>    
                <img class="image-container" src="lzpl.png" alt="lzpl">
                </div>
                <div>
                <div class="label"> Level Nearest, Point NN </div>
                <img class="image-container" src="lnpn.png" alt="lnpn">
                </div>
                 <div>
                 <div class="label"> Level Nearest, Point Bilinear </div>
                <img class="image-container" src="lnpl.png" alt="lnpl">
                </div>
        </div>
        <p class="caption">Comparing various level and point sampling combinations. Having a high complexity of both seems to smooth out edges the best, leading to higher anti-aliasing power.
</p>
            
        </section>
        
        
        <!-- Add more sections as needed -->
        
    </div>
</body>
</html>
