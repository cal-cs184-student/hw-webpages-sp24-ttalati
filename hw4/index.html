<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 184 Homework 4, 
Tanush Talati</title>
    <style>
	.left {
		text-align: left;
	}
        .caption {
            text-align: center;
            font-style: italic;
        }
        .im-container { 
            justify-content: space-between; 
            margin-bottom: -10px;
            flex-wrap: wrap;
            text-align: center;
        }
        .label {
            text-align: center;
        }
        .image-container {
            width: 70%;
        }
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .image {
            width: 100%;
            margin-bottom: -10px;
        }
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            position: fixed;
            bottom: 0;
            width: 100%;
        }
        h1, h2, h3, h4, h5, h6 {
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <header>
        <h1>CS 184 Homework 4: ClothSim</h1>
        <h2> 
Tanush Talati </h2>
    </header>
    
    <div class="container">
        <h2>Overview</h2>

	<p>
	In this assignment, I implemented a real-time cloth simulator, which relied on a spring-mass based system to physically simulate the movement of physical objects such as cloth. The assignment was split into five parts.
	</p>

	<p>The first part required us to actually build out the grid of springs and masses that represent the cloth. This meant connecting neighboring masses with springs to represent shearing, structural, and bending constraints. Such connections are meant to help get realism into the cloth movement. 
	</p>

	<p>
	In the second part, the structure of connections that were formed was leveraged to actually implement the machinery that would determine the location of the different point masses at different time periods. In other words, it dealt with implementing the simulation component. To do this, first I had to calculate the total force acting on a particular mass. This would be the sum of all the different enabled constraint forces acting on the spring. Next we had to utilize verlet integration, a method to approximate dynamics governed by differential equations to deduce the location and velocity of the point mass at the next time step. Since this method of simulation is unstable in nature, the final step involved bounding the distance of a point mass away from its rest position, which was simply done by checking if it was more that 10% away from its rest position and if it was, we would manually adjust the position to be at the threshold mark.
	</p>
    <p>
	    The third part helped implement a collision detection and handling system. Basically the second part gave a working simulator when only one object existed in the world but with multiple objects, we need to make sure that objects do not go through each other and hence need to detect collisions, because if they occur we can adjust the location of the mass to stay above the collided surface. We only dealt with collisions against two different objects: spheres and planes. The only difference between the two was the process in determining if a collision occurred due to the different geometry of the two surfaces. But after that, if a collision occurred, in both cases a correction vector, allowing the object to stay slightly above the collided point was applied and the new position would be this position scaled by 1 - frictional force.
    </p>
    
    <p>
  The fourth part dealt with self-collisions. This would allow the simulation to exhibit complex geometry such as folding on itself when the cloth is dropped for instance. This was implemented by first creating a hashmap where all masses within a rectangular-prism volume would get hashed into the same key and the value would be a list of the objects in the same volume. Once the hashing was done and all the point masses were placed in their respective arrays, we would compare all the pointmasses inside the respective arrays. If two were within some threshold distance, (2 * thickness), a correction vector would be applied to the two point masses to get them at this threshold distance apart (creating spacing). Due to the possibility of multiple masses being this close, the final correction would just be the average of all the corrections on the point mass. 
    </p>

<p>
The last part was a bit different, it had us implement shaders, a quick way to introduce and implement shading schemes that is relatively computationally cheap. Multiple shaders were implemented. First we started with the basic diffuse shading, where light is reflected equally in all directions. Then, more complex versions were implemented, namely the blinn-phong shading model, texture mapping, displacement and bump-mapping, and environment mapped reflections.
</p>
	    
        <section>
            <h3>Part 1: Masses and Springs</h3>
            
            
            <p>In this part the grid of point masses and the springs that interconnect the various point masses was implemented. This spring-mass representation of the cloth allows us to simulate physics that is used to approximate and simulate realistic cloth movement. The cloth was parameterized by num_width_points and num_height_points, representing the number of masses along the width and height of the cloth respectively. The third dimension was either set to 1 or a random number between -1/1000 and 1/1000 depending on the plane the cloth existed in. Furthermore if a cloth point mass was pinned, hence it would not be moving, that was set as well with a boolean flag.           
            </p>

		<p>Once the grid of point masses was constructed, springs defining the interconnections between the point masses were created. There were three types of spring, representing different constraints: structural, shearing, and bending. Depending on the type of constraint, the spring connected different point masses according to how it was detailed out the spec. Once these mainly bookmarking implementations were complete, a mesh of connections could be seen. These meshes are shown below.
            </p>

		<p>Below is the cloth wireframe showing all the connections both bending and steady behavior.        
            </p>

	    <div class='im-container'>
                <div>
                <div class="label"> Grid-View </div>
                <img class="image-container" src="grid.png" alt="iter0">
                </div>
                <div>  
                <div class="label"> Showing the bending </div>    
                <img class="image-container" src="bending.png" alt="iter1">
                </div>
            </div>

	<p class="caption">Showing the grid view with all three constraints, representing working connection of point-masses using springs.
</p>
<p class="left">
		Now I just show some of the constraints individually.
	</p>

		 <div class='im-container'>
                <div>
                <div class="label"> All Three Constraints </div>
                <img class="image-container" src="three.png" alt="iter0">
                </div>
                <div>  
                <div class="label">Only Shearing Constraints </div>    
                <img class="image-container" src="shearing.png" alt="iter1">
                </div>
		<div class="label">No Shearing (Bending and Structural) </div>    
                <img class="image-container" src="noshearing.png" alt="iter1">
                </div>
            </div>
			<p class="caption">Showing the connections when turning off some contraints the the view.
</p>
        </section>
        
        <section>
            <h3>Part 2: Bounding Volume Hierarchy </h3>
            <p> As noted in the overview, naively when testing whether or not a ray intersects a particular primitive, all the primitives are looped through and then we check for intersection on each one. This is linear in the order of the number of primitives, which could be pretty slow. This is where the BVH comes into play, a tree structure that helps cut the intersection test time to a logarithmic order, much more amenable to scaling. 
	    </p>

		<p>Constructing the BVH followed this process: namely the actual iterators for the primitives only exist at the leaf nodes. A node of the tree would become a leaf if the number of ongoing primitives were less than or equal to the maximum leaf size. This would basically function as our stopping point for recursion (or iteration as implemented in the extra credit) for creating new nodes in our BVH. The bulk of the logic is as follows for deciding the set of primitives that would belong to the right and left BVH nodes respectively. Basically given the root BVH node with a start and end iterator representing all the primitives that its bounding box covers, our job is to split it into two separate bounding boxes of primitives. Note that unlike KD-trees the bounding boxes are not disjoint but rather the primitives that are split are disjoint in a BVH. The splitting is determined by a heuristic. Initially just to check the functionality of the BVH I just split on the mean centroid of the x-axis but later I implemented a better heuristic. The heuristic that was used is as follows: we check all the three axis (x, y, z) and our candidate split point for each of the axis is the mean centroid location (so the mean of all the centroid locations for all the primitives along the axis of interest). On each of these candidates, I would check the surface area heuristic score. As introduced in lecture, essentially the score tries to minimize the chance of needless intersection tests. A heuristic for that determination is simply calculating the surface area of the bounding boxes of each split and multiplying it by the number of primitives in the split. The idea being that the number of intersection tests is proportional to the surface area of our split so minimizing it will reduce the intersections in the long run since the surface area encodes the probability of a ray hitting a bounding box node. Out of all the three axis, I kept the best (lowest) scoring split.  Once the split was determined, the goal was to actually add new child nodes and connect pointers to the parent. The parent node is confirmed not to be a leaf so it can now hold the pointers to the two new child pointers we create. Now the child nodes need an updated view of the iterators. These iterators were determined by editing the pointers that our iterators pointed to using a for loop that went through the new split primitives in order. This took a lot of time since initially just had one stack and set the iterators in the nodes but because the variables were declared as constants it would not allow me. I had to then use a 3-stack method (for the extra credit). Now that we have updated pointers and field variables, we can rely on the recursive base case to stop further iterating, once we have few primitives or no more actual splits (this was determined by checking if the array size of either the left or right split was zero, implying that we did not actually split [not possible with the heuristics we used]).	
		</p>

		<p>
		Once the BVH construction is complete, whenever we have an intersection task, the data structure is used as a way to get fast, logarithmic search. Namely two methods intersect and has_intersect were constructed. has_intersect just concerns with if there is an intersection not where it is so one can imagine the logic is the same as intersect except we can return early since we are not worried about finding the first intersection, rather just knowing if there is an intersection or not. Therefore, I will talk about the intersect method. Here we first check if the ray even intersects the root node, if not we know it cannot intersect any of the children either so it allows us to not do any redundant intersection tests. If we do have a bounded box intersection, we simply check if the node is a child, because if it is then we check the intersections on the primitives, which was implemented in the previous part and we will get the first primitive that intersected. If not we run this recursive intersection method on the two children, and the recursive step will guarantee us knowing whether or not an intersection has occurred and if it has the intersection structure will have the right populated fields. 	
		</p>

		<p>Here are large dae files that would take a painfully long time to render with no acceleration. With acceleration all of them took easily less than .1 seconds, showcasing the massive speedup.
		</p>
       
		<div class='im-container'>
                <div>
                <div class="label"> Dragon </div>
                <img class="image-container" src="dragon.png" alt="iter0">
                </div>
                <div>  
                <div class="label"> Max Planck </div>    
                <img class="image-container" src="max.png" alt="iter1">
                </div>
		<div>  
                <div class="label"> Lucy </div>    
                <img class="image-container" src="lucy.png" alt="iter2">
                </div>
		<div>
		<div class="label"> Wall-E </div>    
                <img class="image-container" src="empty.png" alt="iter2">
                </div>
            </div> 

	<p class="caption">All of these files are quite large so trying to render them without a BVH would take a lot longer.
</p>

	<p class="left">
		In this section I give some of the runtimes of attempting to render without BVH and with BVH, showcasing the large speedup the data structure brings. 
	</p>

	<table>
                <thead>
                    <tr>
                        <th>File Name</th>
                        <th>Non-BVH (seconds)</th>
			<th>BVH (seconds)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>cow.dae</td>
                        <td>3.223</td>
			<td>0.053</td>
                    </tr>
                    <tr>
                        <td>maxplanck.dae</td>
                        <td>36.4196</td>
			<td>0.071</td>
                    </tr>
		  <tr>
                        <td>dragon.dae</td>
                        <td>87.016</td>
			<td>0.088</td>
                    </tr>
                </tbody>
            </table>
<p>It is pretty clear that having the BVH structure makes a large difference. The magnitude is 100x better even for smaller renders and obviously the speedup differential will continue to increase as the renders get larger and larger as the underlying difference between log x vs x in growth will only continue to increase. This large gain in speedup is massive because it allows us to do more intensive and more intersection tests in general and is one reason that the render times for having multiple samples and also multiple light samples for each pixel sample and then also adding the fact that we will have more bounces instead of just direct lighting is even possible, otherwise if our intersection tests were too slow, at each step we are adding this multiplicative factor of iterations and all of them depend on a fast intersection resolution protocol.</p>
	
        </section>

        <section>
            <h3>Part 3: Direct Illumination</h3>
<p> In this part direct illumination was implemented. Specifically, both one-bounce lighting and zero-bounce lighting tracing was implemented. Lastly, two types of direct illumination were implemented and compared: uniform hemisphere sampling and light importance sampling.</p>
		<p>Before implementing any sort reflectance measure, first we need to define the BSDF of our primitives. In this assignment all of the surfaces were taken as diffuse and hence we needed to implement the BSDF function that would reflect the irradiance at a point equally in all directions across the hemisphere. This is important in knowing how much light travels toward the camera or later toward a bounce in the direction that is being sampled. Once the BSDF function was implemented the first thing done was to implement the zero-bounce illumination. This means that only emissive sources would render. In the case of the assignment, the light source at the top of the Cornell box. This was pretty easy to implement as when the zero_bounce function is called, we simply return the object’s emission, which is a field variable. Upstream, that will just get set to the pixel’s radiance (or be a factor in it when we do multiple bounces).
		</p>
		<p>Once the bare was finished, the task was to actually implement direct lighting. In this case we make use of the function: one_bounce_radiance. Basically this returns the amount of light reflected on the particular intersection point as a result of the direct illumination of some light source. The implementation of the function was such, we repeat the same process for the user-specified number of samples per light area. And just to be consistent with the other method of direct light sampling, we multiply the number of samples by the number of light sources so in both cases we sample equally. But back to talking about uniform hemisphere sampling, inside the loop we already know the intersection point so our goal is to take the number of sample light measurements from that point by casting that many rays from that point onto a hemisphere. In this first method, the casted ray is uniformly sampled. So in summary we have num_samples * light_sources samples that we take of a casted ray from the intersection point. Now we measure where that casted ray intersects out in the world. This is a bit tricky, since first the casted ray that we sampled was in object space but intersections are in world space, so we convert the ray into world space and then use our intersection machinery we developed to see if it intersects an object in our world. Another thing we want to be careful about is to set the minimum value for the intersection to be a bit greater than 0 because we do not want the intersection with the point we are casting the ray from to count. Once we have received our t value for the intersection (and a boolean indicating whether or not intersection occurred), if the intersection in the world occurred and because we know where, through the mechanics of the light transport we know that the amount of light emitted by the intersecting object will be the amount transferred into our object. Then using the BSDF function we can determine finally how much of that light is reflected onto the camera. The idea of sampling is a Monte-Carlo approximation of the integral of all the light in the whole world, and because we are taking samples to approximate that average integral value, we must make sure to normalize by the probability of casting that sample. In this case it was uniform on the hemisphere hence 1/2*pi. The sum that is getting accumulated is the total light transferred to the camera, which is just the product of the bsdf function * light transferred from the ray cast from intersection point to intersection point * cos (angle between casted ray and surface normal). The cosine is there because that gives the scalar light intensity quantity into the surface. At the end we divide the accumulating sum (our estimate of the integral) by the number of samples. Also if there was no intersection from the casted ray notice how the accumulating sum just gets added with 0 which is what the appropriate behavior should be.
		</p>
		<p>The previous section covered uniform hemisphere sampling, where once again we basically casted rays from the point of intersection uniformly in a hemisphere. But as I show in the pictures, this leads to a lot of noise as many directions just do not end up going to a light source, resulting in point darkness throughout the photos. Basically the idea behind light sampling is that we first iterate through all the light sources and for each light source we take num_samples ray samples uniformly in the region of the light source. This way we are guaranteed to be casting a ray toward a light source (this does not guarantee a hit of the light source because as we will see the light source could be blocked). Another tricky part is that some light sources are point light sources, that means only one direction exists. This is okay we can simply remove redundant computation by taking our sampled light and multiplying it by num_samples. The rest of the machinery should be similar. Now I will go through some implementation details that we need to be careful about when implementing this feature. When we iterate though each of the light sources first see if it is a point light source or not (if it is the former we can save computation as discussed above). Now that we have a sampled direction, we do the same as in the previous part of converting the direction to a world-space ray. We already know that this ray will hit the light eventually because we sampled in that way, but it may potentially hit another object before, in that case the light source is blocked and we should have a shadow and not add the effect of the light onto the light at our point. To make sure of this we set the bounds of the ray to be minimum a little greater than 0 (same reason as before to make sure we do not intersect the point we are casting the ray from) and the max bounds is a little less than the distance to the ray. Now if there is an intersection it means we DO NOT add the light sampled into our sum since that means we hit something in between. Otherwise just as before we update our Monte-Carlo estimate and make sure to remove the bias as well. At the end similar to the previous case we divide the sum by the total number of samples.
		</p>
		<p>Now I present some renderings using uniform sampling and light-importance sampling.
		</p>

	<div class='im-container'>
                <div>
                <div class="label"> Uniform Sampled Bunny </div>
                <img class="image-container" src="unif_bunny.png" alt="iter0">
                </div>
                <div>  
                <div class="label"> Uniform Sampled Spheres </div>    
                <img class="image-container" src="uniform_bunny.png" alt="iter1">
                </div>
		<div>	
                <div class="label"> Light-Importance Sampled Bunny </div>    
                <img class="image-container" src="bunny_l.png" alt="iter2">
                </div>
		<div>
		<div class="label"> Light-Importance Sampled Spheres </div>    
                <img class="image-container" src="spheres_L.png" alt="iter2">
                </div>
		<div>
		<div class="label"> Light-Importance Sampled Dragon </div>    
                <img class="image-container" src="dragon_L.png" alt="iter2">
                </div>
            </div> 
	<p class="caption">Here are some comparisons of light-importance sampling and uniform sampling. Clearly we see a lot more noise when we do uniform sampling.
</p>
		<p class="left">In comparing the uniform hemisphere sampling and the lighting sampling it is pretty clear that when all parameters are equal (sample numbers and light samples) lighting sampling has a much more clear rendering of the image. Even for pretty high sample values in both pixels and lights, uniform sampling produces quite noisy images, because we are sampling the whole hemisphere from a surface point, which naturally means a lot of non-collisions with light. If we just directly sample toward light sources and then adjust the pdf accordingly we are inherently reducing our sampling noise without compromising the bias of our sample. And we can see the results are much more clear, with very little noise and sharper images since we are directly sampling at a light source. </p>

		<p class="left"> Now I present some images of the effect of the number of light samples on rendering quality using light-importance sampling. We use 1 sample per pixel while increasing the number of light samples. </p>
	
	<div class='im-container'>
                <div>
                <div class="label"> 1 Light Sample </div>
                <img class="image-container" src="1_l.png" alt="flatshading">
                </div>
                <div>  
                <div class="label"> 4 Light Samples </div>    
                <img class="image-container" src="4_l.png" alt="phongshading">
                </div>
		<div>  
		<div class="label"> 16 Light Samples </div>    
                <img class="image-container" src="16_l.png" alt="phongshading">
                </div>
		<div>  
		<div class="label"> 64 Light Samples </div>    
                <img class="image-container" src="64_l.png" alt="phongshading">
                </div>
		<div>  
            </div>
	<p class="caption">We see pretty clearly that using more light samples also helps reduce the noise and increase quality to renders. This meakes sense as it is the same idea behind super sampling. One can also imagine that if we used unifotm sampling instead of direct-light, we would see a lot more noise at each of these light levels.

</p>
	
        </section>

        <section>
            <h3>Part 4: Global Illumination</h3>

<p>In this task global illumination was implemented, namely the one bounce lighting from the previous part was extended to an arbitrary amount of bounces. The direct illumination part is the same as the zero_bounce so that just gets added, but the indirect illumination can take into account light that arrives at an intersection points after up to N bounces. Some bookkeeping that needed to be done is to first update the driver function est_radiance_global_illumination to return the sum of the zeroth and the at_least_one function call instead of one. Now we edit the at_least_one function to implement the up to N bounce functionality. To do this we make use of the depth of the ray. If the depth of the ray is 1 then we simply only return the 1-bounce lighting estimate. Otherwise we have space for more bounces. In this case we want to recurse and add to our 1-bounce estimate any downstream lighting that we received (because we want to get the total, if isAccumulate is off we just return the tail call / final recursive call’s calculation of the lighting which will allow us to only showcase the lighting at the Nth layer of bounce). To recurse we sample a bouncing ray from the intersection point and similar to before we see if that ray hits any other point on the world if it does then we find the lighting reflected from it using the same recursive call to this at_least_one_function. Before doing the recursive call though, we must make sure to decrease the depth of our ray by one to make sure we have a base case. The recursive call will guarantee to return us the total light accumulated from downstream bounces. We simply add that to our direct 1-bounce estimate and that is the total light we return. Similar to before when we sample the downstream bounces, we must adjust the weights by dividing by the pdf to make sure the sample does not become biased. The accumulating light estimate uses the same formula as the previous part. As it is pretty clear, the only major edit is having a recursive function and updating the driver function to call the recursive version of the function instead of just the one_bounce version. </p>

<p>The last part that I had to implement in this task was having global illumination with Russian roulette. The motivation being that having some constant for the number of bounces might in itself introduce sampling bias. To mitigate that we can simply make the number of bounces also non-deterministic. In particular at every bounce (past the first) there is a small chance that we will not sample that bounce. This way we do not have a biased stopping point for light and on average if the termination probability is around 0.3 like suggested we get around 1 + 3.3 bounces.  We can also further configure the maximum depth if we do not want for some reason to go above some specified depth (similar to the configuration in the adaptive sample). Russian roulette capability was added into the existing at_least_one_bounce function by just setting a boolean flag called recur, which would be true if the coin toss was a head (or if this is our first bounce anyway which we want to take if we have set indirect illumination to on). After which the body of the function is the same as before expect we also need to divide by the pdf of the coin toss being true to not prevent bias when we are accumulating our lighting estimate. If we want an absolute maximum stopping point then we can keep the logic from before having the base-case being hit when our running depth counts that we reduce with each recursive call hits 1.</p>

<p>Here I show some images of global illumination working on various scenes</p>
		<div class='im-container'>
                <div>
                <div class="label"> Spheres </div>
                <img class="image-container" src="1024_gl_sphere.png" alt="before">
                </div>
                <div>  
                <div class="label"> Blob </div>    
                <img class="image-container" src="1024_gl_blob.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> Bunny </div>    
                <img class="image-container" src="1024_gl_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> Dragon </div>    
                <img class="image-container" src="1024_gl_dragon.png" alt="after2">
                </div>
		<div> 
            </div>
		<p class="caption">Working global illumination. Pictures rendered with samples per pixel being 1024. </p>

<p class="left">The next set of pictures compare scenes with only direct lighting and then showing the effects of only indirect lighting.</p>

	 <div class='im-container'>
                <div>
                <div class="label"> Direct Lighting on Bunny </div>
                <img class="image-container" src="direct_bunny.png" alt="before">
                </div>
                <div>  
                <div class="label"> Indirect Lighting on Bunny </div>    
                <img class="image-container" src="indirect_bunny.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> Direct Lighting on Spheres </div>    
                <img class="image-container" src="direct_balls.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> Indirect Lighting on Spheres </div>    
                <img class="image-container" src="indirect_spheres.png" alt="after2">
                </div>
		<div> 
            </div>
	<p class="caption">Note how in the indirect lighting the light source is completely dark, which makes sense since indirect lighting accounts for light generating from bounces on other surfaces. </p>

      <p class="left">Now I show the non-accumulating light at increasing depth of bounces (variable m in the code). </p>

	<div class='im-container'>
                <div>
                <div class="label"> m = 0 </div>
                <img class="image-container" src="no_accum_0_bunny.png" alt="before">
                </div>
                <div>  
                <div class="label"> m = 1 </div>    
                <img class="image-container" src="no_accum_1_bunny.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> m = 2 </div>    
                <img class="image-container" src="no_accum_2_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 3 </div>    
                <img class="image-container" src="no_accum_3_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 4 </div>    
                <img class="image-container" src="no_accum_4_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 5 </div>    
                <img class="image-container" src="no_accum_5_bunny.png" alt="after2">
                </div>
		<div> 
            </div>

<p class="caption">Note how there is an apparent decrease in the amount of light affecting the global illumination from subsequent layers. </p>

<p class="left">As mentioned above each subsequent layer we see a decrease in lighting indicating the fact that the effect of that layer on the global light becomes less and less at higher depth levels. We see a particularly sharp drop off between the layer 2 and layer 3, where layer 3 is much darker. This makes sense intuitively as trying to sample light that comes 3 bounces into the camera may not actually hit a light source because there is a good chance one of the rays just hits another part or hits a shadow covering. This probability becomes higher and higher at larger levels, explaining the darkness.
</p>

<p class="left">The set of pictures below show the accumulating global illumination as a function of higher depth levels.
</p>

<div class='im-container'>
                <div>
                <div class="label"> m = 0 </div>
                <img class="image-container" src="accum_0_bunny.png" alt="before">
                </div>
                <div>  
                <div class="label"> m = 1 </div>    
                <img class="image-container" src="accum_1_bunny.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> m = 2 </div>    
                <img class="image-container" src="accum_2_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 3 </div>    
                <img class="image-container" src="accum_3_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 4 </div>    
                <img class="image-container" src="accum_4_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 5 </div>    
                <img class="image-container" src="accum_5_bunny.png" alt="after2">
                </div>
		<div> 
            </div> 
			<p class="caption">As with all the photos in this sections we are using 1024 samples per pixel in these renderings. </p>

<p class="left">As seen in these photos if we have higher depths of global illumination our images look brighter, which makes sense because each level is adding some degree of light. However the rate of change of this brightness is also begins to fall off at higher depth levels. We could have predicted this after looking at the previous set of images that showed decreasing light at non-accumulating depth levels.
</p>

			<p class="left">Now I will show the renderings using Russian-Roulette rendering with increasing depth levels.
</p>
	<div class='im-container'>
                <div>
                <div class="label"> m = 0 </div>
                <img class="image-container" src="russian_0_bunny.png" alt="before">
                </div>
                <div>  
                <div class="label"> m = 1 </div>    
                <img class="image-container" src="russian_1_bunny.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> m = 2 </div>    
                <img class="image-container" src="russian_2_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 3 </div>    
                <img class="image-container" src="russian_3_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 4 </div>    
                <img class="image-container" src="russian_4_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 100 </div>    
                <img class="image-container" src="russian_100_bunny.png" alt="after2">
                </div>
		<div> 
            </div> 

			<p class="caption">Russian Roulette rendering with increasing max_depth levels. Samples per pixel was still 1024. </p>	
		<p class="left">We see that past around m = 3 the images start looking very similar, especially when comparing m = 4 and m = 100. This makes sense as the parameter used to render these was p = 0.35, which makes our expected bounces around 1 + 3 = 4. The one because we hard code always making one bounce and then apply Russian Roulette.
</p>	

			<p class="left"> Now I am going to compare the renderings when increasing number of pixels per sample while keeping light samples constant at 4.
</p>	
			<div class='im-container'>
                <div>
                <div class="label">  m = 1 </div>
                <img class="image-container" src="sample_1_bunny.png" alt="before">
                </div>
                <div>  
                <div class="label"> m = 2 </div>    
                <img class="image-container" src="sample_2_bunny.png" alt="after1">
                </div>
		<div>  
		<div>  
                <div class="label"> m = 4 </div>    
                <img class="image-container" src="sample_4_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 8 </div>    
                <img class="image-container" src="sample_8_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 16 </div>    
                <img class="image-container" src="sample_16_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 64 </div>    
                <img class="image-container" src="sample_64_bunny.png" alt="after2">
                </div>
		<div> 
		<div>  
                <div class="label"> m = 1024 </div>    
                <img class="image-container" src="sample_1024_bunny.png" alt="after2">
                </div>
		<div> 
            </div> 

			<p class="caption"> Increasing number of samples per pixel while keeping light samples constant. </p>	
			
			<p class="left"> Pretty evidently increasing the number of samples per pixel reduces artifacts that come from noise, our pixel colors become a lot more smooth at higher sample levels and we do not see the dotted-ness that is present at lower sampling levels.
</p>	
			
        </section>

        <section>
             <h3>Part 5: Adaptive Sampling</h3>
            <p> The idea behind adaptive sampling is that we can reduce the noise in our rendered images by only stopping sampling once we are confident about the light estimate at the point. Specifically we implement a 95% confidence interval. In implementing this most of the work is in the outermost function: raytrace_pixel. Here is where we take different ray samples to estimate lighting. Basically the idea is as we are taking these ray samples of lighting we accumulate the mean lighting and the standard deviation of the lighting samples. If the mean * 1.96 *(standard deviation of lighting estimate) / sqrt (number of running samples) represents only a 5% of a deviation from our running estimate mean, then we can claim confident convergence of our mean value and assign that to the pixel’s radiance value. One may notice that having to update the current mean and standard deviation of the average is quite an expensive task if done with every ray sample. To mitigate most probably redundant computation, we define a batch size after which we recalculate the estimates and determine convergence. When the batch size is not met we simply keep a streaming count of the current sum of the light samples and the sum of the samples squared because both of these terms help us calculate the end statistics that will lead to convergence. One can also imagine pathological instances where convergence is very tough especially at high frequency locations. In this case we will resort to an absolute maximum number of samples as being the worst-case endpoint for our lighting estimate. Usually this number is quite large since the chance of reaching it is low as convergence will usually be achieved before. 
</p>
<p>
Attached below are two scenes rendered with adaptive sampling with a maximum sample size of 2048.
</p>
            <div class='im-container'>
                <div>
                <div class="label">Bunny Image </div>
                <img class="image-container" src="adapt_bunny.png" alt="tp">
                </div>
                <div>  
                <div class="label"> Bunny Sampling Rate Image</div>    
                <img class="image-container" src="adapt_bunny_rate.png" alt="tpflip">
                </div>
                <div>
                <div class="label"> Spheres Image </div>
                <img class="image-container" src="adapt_balls.png" alt="tpflips">
                </div>
		    <div>
                <div class="label"> Spheres Sampling Rate Image </div>
                <img class="image-container" src="adapt_balls_rate.png" alt="tpflips">
                </div>
            </div>
            <p class="caption">Showing the rendered scenes using adaptive sampling and the associated sample rates. Notice how high frequency areas tend to have higher sampling rates. These images were rendered using depth of 5 and 1 sample per light.
</p>

		
        </section>

        <section>
             <h3>Part 6: Extra Credit</h3>
		<p>For extra credit I implemented two different changes, both to the BVH to make it run faster.</p>
		<h4>Extra Credit 1: Non-Recursive BVH</h4>

        <p>
	The first optimization I did was to implement the BVH using an iterative approach instead of a recursive one. In order to do this, I used a stack and a while loop. Once the stack was empty, I would know that I had finished making the new nodes and the entire BVH structure was ready. When I needed to make new children nodes (when one of the stopping points had not been met yet like too many primitives existing in the bounding box) I would create the two children nodes and then push them to the stack. This is the same thing as creating two children nodes and recursing on them in DFS order but we save the need to create a new function call and instead can do everything inside the original function. One thing that was particularly tough (maybe this would be obvious for someone more adept with C++) was the fact that the fields in the BVH node structure were defined to be constants and initially I was also placing the updated iterators there but that would not work since one can imagine you need to reupdate the iterators as you make downstream edits. I got around this by adding two extra stacks that held the start and end iterators for each of the on-going nodes. I am still not sure if this is the best way to get around the constants but I can imagine right now it is one place that can be updated to make this even faster.
	</p>
		<p>When noticing the difference between recursive implementation and an iterative on, there was a clear around 18% boost in timing performance, with the rate being more accurate for larger renders. </p>
<h4>Extra Credit 2: Surface Area Heuristic for BVH Node Split</h4>
		<p>The second extra credit item I worked on was applying a different heuristic to the split the primitives into two separate nodes. Specifically, I implemented the surface area based split that was introduced in lecture. Specifically we hypothesize that the hit rate of a node is the proportional to the ratio of the surface area of the outer bounding box to the inner bounding box and hence we want to minimize the hit rate to minimize expected hits and the expected time on intersection. Implementing the surface area heuristic was simple, I just needed to add code that added this score and then I compared the split across 3 different axis, x, y, z. I only split on the mean centroid but one could imagine extending this out to also check each partition between the primitives and seeing if the split was better. But given the meshes we were dealing with, I did not think it would make too much of a difference on the end result given the amount of overhead in preprocessing that split. But the surface area split showed significant improvements over the other split I was previously using. Specifically, having a nicely configured split was showcasing around a drop of 40% in the amount of intersection tests per ray (from around 7 on average to 4). This is significant as that has a huge repercussions in reducing the run times of our tracing times.
		</p>

		
        </section>
			
<!-- 
			<table>
                <thead>
                    <tr>
                        <th>Column 1</th>
                        <th>Column 2</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Data 1</td>
                        <td>Data 2</td>
                    </tr>
                    <tr>
                        <td>Data 3</td>
                        <td>Data 4</td>
                    </tr>
                </tbody>
            </table> -->
        
        <!-- Add more sections as needed -->
        
    </div>
</body>
</html>
